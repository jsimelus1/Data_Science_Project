{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Computational Linguistics over Reddit Data\n",
    "\n",
    "For this project we are going to ingest Reddit posts, process the data and perform computational linguistics over the Reddit posts.\n",
    "\n",
    "This project will build off of some work you have previously done. However, beyond that exercise of processing and cataloging the feeds, in this instance you will access the referenced Reddit post and perform computational linguistics over the post itself.\n",
    "\n",
    "![DataScraper_To_NLP.png MISSING](../images/DataScraper_To_NLP.png)\n",
    "\n",
    "---\n",
    "\n",
    "### From the site:\n",
    "\n",
    "reddit: https://www.reddit.com/  \n",
    "Reddit gives you the best of the Internet in one place. Get a constantly updating feed of breaking news, fun stories, pics, memes, and videos just for you.\n",
    "\n",
    "\n",
    "### From Wikipedia:\n",
    "Reddit is an American social news aggregation, web content rating, and discussion website. \n",
    "Registered members submit content to the site such as links, text posts, and images, \n",
    "which are then voted up or down by other members. \n",
    "Posts are organized by subject into user-created boards called \"subreddits\", \n",
    "which cover a variety of topics including news, science, movies, video games, music, books, fitness, food, and image-sharing. \n",
    "Submissions with more up-votes appear towards the top of their subreddit and, if they receive enough votes, ultimately on the site's front page. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Posting:\n",
    "\n",
    "The below link is an example post from someone that was tinkering with sentiment analysis; specifically they looked at the text of [Moby Dick](https://en.wikipedia.org/wiki/Moby-Dick).\n",
    "\n",
    "**Spoiler:** The conclusion was that the book is rather negative in sentiment.\n",
    "It is after all, about vengeance!\n",
    "\n",
    "https://www.reddit.com/r/LanguageTechnology/comments/9whk23/a_simple_nlp_pipeline_to_calculate_running/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From: https://www.redditinc.com/\n",
    "![REDDIT_About.png MISSING](../images/REDDIT_About_latest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example Code:\n",
    "\n",
    "In this exercise, we will be using Reddit API for fetching the latest messages. We can also fetch recent posts from Reddit using web feeds (check [here](./rss-feeds.ipynb)), but it seems our IP got banned for excessive requests to Reddit over the last few days. So we will be using Reddit API for which you are required to create your Reddit account and an app. \n",
    "\n",
    "Follow [this article](https://gilberttanner.com/blog/scraping-redditdata) to create your credentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Reddit API\n",
    "\n",
    "For fetching Reddit data using API, we will be using a Python wrapper to Reddit API: [PRAW: The Python Reddit API Wrapper](https://github.com/praw-dev/praw)\n",
    "\n",
    "Documentation: https://praw.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.4.0 of praw is outdated. Version 7.7.1 was released Tuesday July 11, 2023.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import datetime\n",
    "\n",
    "reddit = praw.Reddit(client_id='3yY7G906Udk--SEIM6qbug', \n",
    "                     client_secret='0t8wt0AZBFPBdIXTIlEuFESK4XVApA',\n",
    "                     user_agent='James_Webscrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_posts = reddit.subreddit('datascience').hot(limit=10)\n",
    "all_posts = list(hot_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 1fxz22n\n",
      "title : Weekly Entering & Transitioning - Thread 07 Oct, 2024 - 14 Oct, 2024\n",
      "url : https://www.reddit.com/r/datascience/comments/1fxz22n/weekly_entering_transitioning_thread_07_oct_2024/\n",
      "author : AutoModerator <class 'str'>\n",
      "score : 1 <class 'int'> \n",
      "subreddit : datascience <class 'praw.models.reddit.subreddit.Subreddit'> \n",
      "num_comments : 56\n",
      "body :  \n",
      "\n",
      "Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n",
      "\n",
      "* Learning resources (e.g. books, tutorials, videos)\n",
      "* Traditional education (e.g. schools, degrees, electives)\n",
      "* Alternative education (e.g. online courses, bootcamps)\n",
      "* Job search questions (e.g. resumes, applying, career prospects)\n",
      "* Elementary questions (e.g. where to start, what next)\n",
      "\n",
      "While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
      "created : 1728273691.0\n",
      "link_flair_text : None\n"
     ]
    }
   ],
   "source": [
    "for post in all_posts:\n",
    "    print(f\"id : {post.id}\")\n",
    "    print(f\"title : {post.title}\")\n",
    "    print(f\"url : {post.url}\")\n",
    "    print(f\"author : {str(post.author)} {type(str(post.author))}\")\n",
    "    print(f\"score : {post.score} {type(post.score)} \")\n",
    "    print(f\"subreddit : {post.subreddit} {type(post.subreddit)} \")\n",
    "    print(f\"num_comments : {post.num_comments}\")\n",
    "    print(f\"body : {post.selftext}\")\n",
    "    print(f\"created : {post.created}\")\n",
    "    print(f\"link_flair_text : {post.link_flair_text}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Reddits\n",
    "\n",
    "As described above, sub-reddits are communities organized around particular topics.\n",
    "\n",
    "Some example sub-reddits:\n",
    " * https://www.reddit.com/r/datascience/\n",
    " * https://www.reddit.com/r/MachineLearning/\n",
    " * https://www.reddit.com/r/LanguageTechnology/\n",
    " * https://www.reddit.com/r/NLP/\n",
    " * https://www.reddit.com/r/Python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Tasks\n",
    "\n",
    "## Part I: Data Acquisition and Loading \n",
    "1. Choose a subreddit of your choice. Preferably something of interest to you. \n",
    "1. Conceptualize a database design that can collect the data.\n",
    "    * Make sure your items (posts) are unique and not duplicated!\n",
    "    * Make sure you capture at least title, author, subreddit, tags, title link, and timestamp\n",
    "    * Along with the metadata, capture all the text into one or more data field(s) suitable for information retrieval\n",
    "    * Write triggers for auto updates of IR related fields\n",
    "    * Add index (either GIN or GiST) for the IR related fields\n",
    "    * Additionally, design a field to hold:\n",
    "        * Sentiment\n",
    "1. Implement the database in your PostgreSQL schema\n",
    "1. Implement cells of Python Code that \n",
    "    * collect the latest posts from a subreddit of your choice (**should be text-dominant not image/video**) and collect at least 500 posts (if possible), \n",
    "    * processes the messages to extract metadata, \n",
    "    * process the text for IR, and \n",
    "    * perform computational linguistics (i.e, extract sentiment scores), \n",
    "    * then insert the data into your database.\n",
    "1. After you have loaded data from a subreddit, choose a few more subreddits and load those!\n",
    "\n",
    "## Part II: Analytics \n",
    "\n",
    "1. Write some test queries following the text vectors from Module 7.\n",
    "1. Produce **interesting visualizations** of the linguistic data.\n",
    "    * Try to look for trends (within a subreddit) and and variations of topics across subreddits\n",
    "    * Some comparative plots across feeds\n",
    "1. Write a summary of your findings!\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data Acquisition and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Design your database\n",
    "\n",
    "Conceptualize a database design that can collect the data.\n",
    "* Make sure your items (posts) are unique and not duplicated!\n",
    "* Make sure you capture at least title, link, author, subreddit, tag/flair, and timestamp\n",
    "* Capture all the body text into fields suitable for information retrieval\n",
    "* Write triggers for auto updates of IR related fields\n",
    "* Add index (either GIN or GiST) for the IR related fields\n",
    "* Additionally, design a field to hold:\n",
    "    - Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weekly Entering &amp; Transitioning - Thread 07 Oc...</td>\n",
       "      <td>1</td>\n",
       "      <td>1fxz22n</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>56</td>\n",
       "      <td>\\n\\nWelcome to this week's entering &amp; transit...</td>\n",
       "      <td>2024-10-06 23:01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I created a 6-week SQL for data science roadma...</td>\n",
       "      <td>372</td>\n",
       "      <td>1fzswk1</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>34</td>\n",
       "      <td>I created this roadmap to guide you through ma...</td>\n",
       "      <td>2024-10-09 09:36:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SQL queries that group by number</td>\n",
       "      <td>10</td>\n",
       "      <td>1g01l33</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>31</td>\n",
       "      <td>I wanted to know if generally people use group...</td>\n",
       "      <td>2024-10-09 15:41:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Need help on analysis of AI performance, compu...</td>\n",
       "      <td>6</td>\n",
       "      <td>1fzw393</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/gallery/1fzw393</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2024-10-09 11:50:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I linked AI Performance Data with Compute Size...</td>\n",
       "      <td>0</td>\n",
       "      <td>1g07nvw</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/gallery/1g07nvw</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>2024-10-09 20:27:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which position should I join? (Palantir Develo...</td>\n",
       "      <td>53</td>\n",
       "      <td>1fzh90w</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>58</td>\n",
       "      <td>I have recently received two offers from two d...</td>\n",
       "      <td>2024-10-08 21:26:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Does business dictate what models or methodolo...</td>\n",
       "      <td>6</td>\n",
       "      <td>1fznew0</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>6</td>\n",
       "      <td>Hey guys,\\n\\nI am working on a forecasting pro...</td>\n",
       "      <td>2024-10-09 04:30:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>does anyone use Posit Connect?</td>\n",
       "      <td>16</td>\n",
       "      <td>1fzi4an</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>12</td>\n",
       "      <td>I'm curious what companies out there are using...</td>\n",
       "      <td>2024-10-08 22:13:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A guide to passing the A/B test interview ques...</td>\n",
       "      <td>920</td>\n",
       "      <td>1fyrawz</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>77</td>\n",
       "      <td>Hey all,\\n\\nI'm a Sr. Analytics Data Scientist...</td>\n",
       "      <td>2024-10-07 23:08:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Nobel Prize in Physics 2024 was awarded to...</td>\n",
       "      <td>59</td>\n",
       "      <td>1fz6o4i</td>\n",
       "      <td>datascience</td>\n",
       "      <td>/r/math/comments/1fyzz6t/the_nobel_prize_in_ph...</td>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td>2024-10-08 13:19:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score       id  \\\n",
       "0  Weekly Entering & Transitioning - Thread 07 Oc...      1  1fxz22n   \n",
       "1  I created a 6-week SQL for data science roadma...    372  1fzswk1   \n",
       "2                   SQL queries that group by number     10  1g01l33   \n",
       "3  Need help on analysis of AI performance, compu...      6  1fzw393   \n",
       "4  I linked AI Performance Data with Compute Size...      0  1g07nvw   \n",
       "5  Which position should I join? (Palantir Develo...     53  1fzh90w   \n",
       "6  Does business dictate what models or methodolo...      6  1fznew0   \n",
       "7                     does anyone use Posit Connect?     16  1fzi4an   \n",
       "8  A guide to passing the A/B test interview ques...    920  1fyrawz   \n",
       "9  The Nobel Prize in Physics 2024 was awarded to...     59  1fz6o4i   \n",
       "\n",
       "     subreddit                                                url  \\\n",
       "0  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "1  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "2  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "3  datascience             https://www.reddit.com/gallery/1fzw393   \n",
       "4  datascience             https://www.reddit.com/gallery/1g07nvw   \n",
       "5  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "6  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "7  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "8  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
       "9  datascience  /r/math/comments/1fyzz6t/the_nobel_prize_in_ph...   \n",
       "\n",
       "   num_comments                                               body  \\\n",
       "0            56   \\n\\nWelcome to this week's entering & transit...   \n",
       "1            34  I created this roadmap to guide you through ma...   \n",
       "2            31  I wanted to know if generally people use group...   \n",
       "3             4                                                      \n",
       "4             5                                                      \n",
       "5            58  I have recently received two offers from two d...   \n",
       "6             6  Hey guys,\\n\\nI am working on a forecasting pro...   \n",
       "7            12  I'm curious what companies out there are using...   \n",
       "8            77  Hey all,\\n\\nI'm a Sr. Analytics Data Scientist...   \n",
       "9            13                                                      \n",
       "\n",
       "              created  \n",
       "0 2024-10-06 23:01:31  \n",
       "1 2024-10-09 09:36:10  \n",
       "2 2024-10-09 15:41:19  \n",
       "3 2024-10-09 11:50:22  \n",
       "4 2024-10-09 20:27:28  \n",
       "5 2024-10-08 21:26:32  \n",
       "6 2024-10-09 04:30:56  \n",
       "7 2024-10-08 22:13:56  \n",
       "8 2024-10-07 23:08:05  \n",
       "9 2024-10-08 13:19:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = []\n",
    "ds_subreddit = reddit.subreddit('datascience')\n",
    "\n",
    "for post in ds_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments,\n",
    "                  post.selftext, datetime.datetime.fromtimestamp(post.created)])\n",
    "\n",
    "posts_df = pd.DataFrame(posts, columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments',\n",
    "                                        'body', 'created'])\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "posts_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Your Design here. You can describe your design with text or picture\n",
    "## ---------------------------------------------------------------------------\n",
    "\n",
    "id SERIAL PRIMARY KEY,\n",
    "post_id VARCHAR(255) UNIQUE,\n",
    "title TEXT NOT NULL,\n",
    "author VARCHAR(255),\n",
    "subreddit VARCHAR(255),\n",
    "tags TEXT[],\n",
    "title_link TEXT,\n",
    "timestamp TIMESTAMP,\n",
    "processed_body TEXT,\n",
    "sentiment NUMERIC,\n",
    "ir_field TSVECTOR,\n",
    "processed_body_tsv_gin TSVECTOR,\n",
    "processed_body_tsv_gist TSVECTOR,\n",
    "created_at TIMESTAMP DEFAULT NOW()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Task 2: Implement the database in your PostgreSQL schema\n",
    "\n",
    "You can choose any of the three ways to implement your database. \n",
    "\n",
    "* sql magic \n",
    "* sql terminal \n",
    "* psycopg2 or sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import psycopg2\n",
    "mypasswd = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysso = 'jsmm8'\n",
    "dbname = 'dsa_student'\n",
    "schema = 'jsmm8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "from psycopg2.extensions import adapt, register_adapter, AsIs\n",
    "\n",
    "connection = psycopg2.connect(database = dbname, \n",
    "                              user = mysso, \n",
    "                              host = 'pgsql.dsa.lan',\n",
    "                              password = mypasswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_TABLES = f\"\"\"\n",
    "-------------------------\n",
    "-- Basic Table \n",
    "-------------------------\n",
    "DROP TABLE IF EXISTS {schema}.data_science_posts;\n",
    "\n",
    "CREATE TABLE {schema}.data_science_posts(\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    post_id VARCHAR(255) UNIQUE NOT NULL,\n",
    "    title TEXT NOT NULL,\n",
    "    author VARCHAR(255) NOT NULL,\n",
    "    subreddit VARCHAR(255) NOT NULL,\n",
    "    tags TEXT[],\n",
    "    title_link TEXT NOT NULL,\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    processed_body TEXT,\n",
    "    sentiment VARCHAR(50),\n",
    "    ir_field TSVECTOR,\n",
    "    created_at TIMESTAMP DEFAULT NOW()\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(CREATE_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trigger_function_query = '''\n",
    "CREATE OR REPLACE FUNCTION update_ir_field()\n",
    "RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "    NEW.ir_field := to_tsvector('english', COALESCE(NEW.title, '') || ' ' || COALESCE(NEW.post_body, ''));\n",
    "    RETURN NEW;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(CREATE_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trigger_query = '''\n",
    "CREATE TRIGGER ir_field_update\n",
    "BEFORE INSERT OR UPDATE ON data_science_posts\n",
    "FOR EACH ROW\n",
    "EXECUTE FUNCTION update_ir_field();\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(CREATE_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_query = '''\n",
    "CREATE INDEX IF NOT EXISTS ir_field_idx ON data_science_posts USING GIN(ir_field);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(CREATE_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY = f\"\"\"\n",
    "SELECT table_schema, table_name, column_name, data_type\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = '{schema}' AND table_name = 'data_science_posts';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(CREATE_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>id</td>\n",
       "      <td>integer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>post_id</td>\n",
       "      <td>character varying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>title</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>author</td>\n",
       "      <td>character varying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>subreddit</td>\n",
       "      <td>character varying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>tags</td>\n",
       "      <td>ARRAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>title_link</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>timestamp without time zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>processed_body</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>character varying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>ir_field</td>\n",
       "      <td>tsvector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jsmm8</td>\n",
       "      <td>data_science_posts</td>\n",
       "      <td>created_at</td>\n",
       "      <td>timestamp without time zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   table_schema          table_name     column_name  \\\n",
       "0         jsmm8  data_science_posts              id   \n",
       "1         jsmm8  data_science_posts         post_id   \n",
       "2         jsmm8  data_science_posts           title   \n",
       "3         jsmm8  data_science_posts          author   \n",
       "4         jsmm8  data_science_posts       subreddit   \n",
       "5         jsmm8  data_science_posts            tags   \n",
       "6         jsmm8  data_science_posts      title_link   \n",
       "7         jsmm8  data_science_posts       timestamp   \n",
       "8         jsmm8  data_science_posts  processed_body   \n",
       "9         jsmm8  data_science_posts       sentiment   \n",
       "10        jsmm8  data_science_posts        ir_field   \n",
       "11        jsmm8  data_science_posts      created_at   \n",
       "\n",
       "                      data_type  \n",
       "0                       integer  \n",
       "1             character varying  \n",
       "2                          text  \n",
       "3             character varying  \n",
       "4             character varying  \n",
       "5                         ARRAY  \n",
       "6                          text  \n",
       "7   timestamp without time zone  \n",
       "8                          text  \n",
       "9             character varying  \n",
       "10                     tsvector  \n",
       "11  timestamp without time zone  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_query(SQL_QUERY, connection)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement cells of Python Code that\n",
    "\n",
    "* collect the latest posts from a subreddit of your choice (should be text-dominant not image/video) and collect at least 500 posts (if possible),\n",
    "* processes the messages to extract id, title, link, author, subreddit, tag/flair, timestamp, etc. \n",
    "* process the text for IR, and\n",
    "* perform computational linguistics (e.g., get sentiment scores)\n",
    "* then insert the data into your database.\n",
    "\n",
    "\n",
    "Notes: \n",
    "* Each call to Reddit API returns 100 entries max. If we set a limit of more than 100, PRAW will handle multiple API calls internally and lazily fetches data. Check obfuscation and API limitation in https://praw.readthedocs.io/en/v3.6.2/pages/getting_started.html. \n",
    "* Develop and test your code with less than 100 messages from a subreddit. Then increase the limit and add few more subreddits. \n",
    "* While loading the table, test with one row \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 3.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from vaderSentiment) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Fetched 151 posts.\n"
     ]
    }
   ],
   "source": [
    "## Your code in this cell\n",
    "## ------------------------\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "\n",
    "connection = psycopg2.connect(database=dbname, \n",
    "                              user=mysso, \n",
    "                              host='pgsql.dsa.lan',\n",
    "                              password=mypasswd)\n",
    "cur = connection.cursor()\n",
    "\n",
    "reddit = praw.Reddit(client_id='3yY7G906Udk--SEIM6qbug', \n",
    "                     client_secret='0t8wt0AZBFPBdIXTIlEuFESK4XVApA',\n",
    "                     user_agent='James_Webscrapper')\n",
    "\n",
    "subreddit_name = 'datascience'\n",
    "ds_subreddit = reddit.subreddit(subreddit_name)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "posts = []\n",
    "for post in ds_subreddit.hot(limit=500):\n",
    "    body_text = post.selftext\n",
    "    sentiment_scores = analyzer.polarity_scores(body_text)\n",
    "    sentiment_label = classify_sentiment(sentiment_scores['compound'])\n",
    "    \n",
    "    posts.append({\n",
    "        'post_id': post.id,\n",
    "        'title': post.title,\n",
    "        'author': str(post.author) if post.author else 'Unknown',\n",
    "        'subreddit': post.subreddit.display_name,\n",
    "        'url': post.url,\n",
    "        'tags': post.link_flair_text,\n",
    "        'created': datetime.datetime.fromtimestamp(post.created),\n",
    "        'processed_body': body_text,\n",
    "        'sentiment': sentiment_label,\n",
    "        'compound': sentiment_scores['compound']\n",
    "    })\n",
    "\n",
    "print(f\"Fetched {len(posts)} posts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.0\n",
      "Neutral: 0.944\n",
      "Positive: 0.056\n",
      "Compound: 0.6696\n",
      "Body:  \n",
      "\n",
      "Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n",
      "\n",
      "* Learning resources (e.g. books, tutorials, videos)\n",
      "* Traditional education (e.g. schools, degrees, electives)\n",
      "* Alternative education (e.g. online courses, bootcamps)\n",
      "* Job search questions (e.g. resumes, applying, career prospects)\n",
      "* Elementary questions (e.g. where to start, what next)\n",
      "\n",
      "While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.814\n",
      "Positive: 0.186\n",
      "Compound: 0.9694\n",
      "Body: I created this roadmap to guide you through mastering SQL in aboutÂ 6 weeksÂ (or sooner if you have the time and are motivated) for free, focusing specifically on skills essential for aspiringÂ Data ScientistsÂ (orÂ Data Analysts)\n",
      "\n",
      "Each section points you to specific resources, mostly YouTube videos and articles, to help you learn each concept.\n",
      "\n",
      "[https://github.com/andresvourakis/free-6-week-sql-roadmap-data-science](https://github.com/andresvourakis/free-6-week-sql-roadmap-data-science)\n",
      "\n",
      "Btw, Iâ€™m a data scientist with 7 years of experience in tech. Iâ€™ve been working with SQL ever since I started my career.\n",
      "\n",
      "I hope this helps those of you just getting started or in need of refresher ðŸ™\n",
      "\n",
      "P.S. Iâ€™m creating a similar roadmap for Python, which hopefully will be ready in a couple of days\n",
      "\n",
      "Negative: 0.093\n",
      "Neutral: 0.907\n",
      "Positive: 0.0\n",
      "Compound: -0.5423\n",
      "Body: I wanted to know if generally people use group by with the numbers instead of the column names. Is this something old school or just bad practice? It makes it so much harder to read.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.058\n",
      "Neutral: 0.895\n",
      "Positive: 0.046\n",
      "Compound: -0.3182\n",
      "Body: I have recently received two offers from two different companies. Same pay and remote.  \n",
      "  \n",
      "Company A (Fortune 500)  \n",
      "Role - **Palantir Application Developer**  \n",
      "In this role, I have to collaborate with senior leaders of the company and develop **Palantir applications** to solve their problems ...and it will be more of a Data Engineer sort of work. However, I am scared as there are not enough palantir-related jobs in the market. The software is costly and is thus not adopted by a lot of organizations. However, the manager is saying that I will get huge exposure to the business as I will be interacting with the senior leadership to understand the business problems.\n",
      "\n",
      "Company B (A health system)  \n",
      "Role - **BI Analyst**  \n",
      "In this role, I will lead the data science collaboration of the health system and there are opportunities to grow into the data science team as well. The company doesn't have a proper data science team thus there is a lot of room I suppose. They use **Dataiku** platform to apply machine learning.\n",
      "\n",
      "Which role should I choose?\n",
      "\n",
      "Negative: 0.082\n",
      "Neutral: 0.872\n",
      "Positive: 0.046\n",
      "Compound: -0.7215\n",
      "Body: Hey guys,\n",
      "\n",
      "I am working on a forecasting project and after two restarts , I am getting some weird vibes from my business SPOC.\n",
      "\n",
      "Not only he is not giving me enough business side details to expand on my features, he is dictating what models to use. For .e.g. I got an email from him saying to use MLR, DT, RF, XGB, LGBM, CatBoost for forecasting using ML. Also, he wants me to use ARIMA/SARIMAX for certain classes of SKUs.\n",
      "\n",
      "The problem seems to be that there is no quantitative KPI for stopping the experimentation. Just the visual analysis of results.\n",
      "\n",
      "For e.g my last experiment got rejected because 3 rows of forecasts were off the mark (by hundreds) out of 10K rows generated in the forecast table. Since the forecast was for highly irregular and volatile SKUs, my model was forecasting within what seemed to be an acceptable error range. If actual sales were 100, my model was showing 92 or 112 etc.\n",
      "\n",
      "Since this is my first major model building on a massive scale, I was wondering if things are like this.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.821\n",
      "Positive: 0.179\n",
      "Compound: 0.5859\n",
      "Body: I'm curious what companies out there are using Posit's cloud tools like Workbench, Connect and Posit Package Manager and if anyone has used them.\n",
      "\n",
      "\n",
      "Negative: 0.041\n",
      "Neutral: 0.853\n",
      "Positive: 0.105\n",
      "Compound: 0.9983\n",
      "Body: Hey all,\n",
      "\n",
      "I'm a Sr. Analytics Data Scientist at a large tech firm (not FAANG) and I conduct about \\~3 interviews per week. I wanted to share my advice on how to pass A/B test interview questions as this is an area I commonly see candidates get dinged. Hope it helps.\n",
      "\n",
      "Product analytics and data scientist interviews at tech companies often include an A/B testing component. Here is my framework on how to answer A/B testing interview questions. Please note that this is not necessarily a guide to design a good A/B test. Rather, it is a guide to help you convince an interviewer that you know how to design A/B tests.\n",
      "\n",
      "**A/B Test Interview Framework**\n",
      "\n",
      "Imagine during the interview that you get asked â€œWalk me through how you would A/B test this new feature?â€. This framework will help you pass these types of questions.\n",
      "\n",
      "**Phase 1: Set the context for the experiment. Why do we want to AB test, what is our goal, what do we want to measure?**\n",
      "\n",
      "1. The first step is to clarify the purpose and value of the experiment with the interviewer. Is it even worth running an A/B test? Interviewers want to know that the candidate can tie experiments to business goals.\n",
      "2. Specify what exactly is the treatment, and what hypothesis are we testing? Too often I see candidates fail to specify what the treatment is, and what is the hypothesis that they want to test. Itâ€™s important to spell this out for your interviewer.Â \n",
      "3. After specifying the treatment and the hypothesis, you need to define the metrics that you will track and measure.\n",
      "   * Success metrics: Identify at least 2-3 candidate success metrics. Then narrow it down to one and propose it to the interviewer to get their thoughts.\n",
      "   * Guardrail metrics: Guardrail metrics are metrics that you do not want to harm. You donâ€™t necessarily want to improve them, but you definitely donâ€™t want to harm them. Come up with 2-4 of these.\n",
      "   * Tracking metrics: Tracking metrics help explain the movement in the success metrics. Come up with 1-4 of these.\n",
      "\n",
      "**Phase 2: How do we design the experiment to measure what we want to measure?**\n",
      "\n",
      "1. Now that you have your treatment, hypothesis, and metrics, the next step is to determine the unit of randomization for the experiment, and when each unit will enter the experiment. You should pick a unit of randomization such that you can measure success your metrics, avoid interference and network effects, and consider user experience.\n",
      "   * As a simple example, letâ€™s say you want to test a treatment that changes the color of the checkout button on an ecommerce website from blue to green. How would you randomize this? You could randomize at the user level and say that every person that visits your website will be randomized into the treatment or control group. Another way would be to randomize at the session level, or even at the checkout page level.Â \n",
      "   * When each unit will enter the experiment is also important. Using the example above, you could have a person enter the experiment as soon as they visit the website. However, many users will not get all the way to the checkout page so you will end up with a lot of users who never even got a chance to see your treatment, which will dilute your experiment. In this case, it might make sense to have a person enter the experiment once they reach the checkout page. You want to choose your unit of randomization and when they will enter the experiment such that you have minimal dilution. In a perfect world, every unit would have the chance to be exposed to your treatment.\n",
      "2. The next step is to conduct a power analysis to determine the number of observations required and how long to run the experiment. You can either state that you would conduct a power analysis using an alpha of 0.05 and power of 80%, or ask the interviewer of the company has standards you should use.\n",
      "   * Iâ€™m not going to go into how to calculate power here, but know that in any ABÂ  test interview question, you will have to mention power. For some companies, and in junior roles, just mentioning this will be good enough. Other companies, especially for more senior roles, might ask you more specifics about how to calculate power.Â \n",
      "3. Next, you need to determine which statistical test(s) you will use to analyze the results. Is a simple t-test sufficient, or do you need quasi-experimental techniques like difference in differences? Do you require heteroskedastic robust standard errors or clustered standard errors?\n",
      "   * The t-test and z-test of proportions are two of the most common tests.\n",
      "   * If your unit of randomization is larger than your analysis unit, you may need to adjust how you calculate your standard errors.\n",
      "   * You might be thinking â€œwhy would I need to use difference-in-difference in an AB testâ€? In my experience, this is common when doing a geography based randomization on a relatively small sample size. Letâ€™s say that you want to randomize by city in the state of California. Itâ€™s likely that even though you are randomizing which cities are in the treatment and control groups, that your two groups will have pre-existing biases. A common solution is to use difference-in-difference. Iâ€™m not saying this is right or wrong, but itâ€™s a common solution that I have seen in tech companies.\n",
      "4. Final considerations for the experiment design:Â \n",
      "   * Are you testing multiple metrics? If so, account for that in your analysis. A really common academic answer is the Bonferonni correction. I've never seen anyone use it in real life though, because it is too conservative. A more common way is to control the False Discovery Rate. You can google this. Alternatively, the book [Trustworthy Online Controlled Experiments](https://amzn.to/4dzXyZP) by Ron Kohavi discusses how to do this (note: this is an affiliate link).Â \n",
      "   * Do any stakeholders need to be informed about the experiment?Â \n",
      "   * Are there any novelty effects or change aversion that could impact interpretation?\n",
      "\n",
      "**Phase 3:** **The experiment is over. Now what?**\n",
      "\n",
      "1. After you â€œrunâ€ the A/B test, you now have some data. Consider what recommendations you can make from them. What insights can you derive to take actionable steps for the business? Speaking to this will earn you brownie points with the interviewer.\n",
      "   * For example, can you think of some useful ways to segment your experiment data to determine whether there were heterogeneous treatment effects?\n",
      "\n",
      "**Common follow-up questions, or â€œgotchasâ€**\n",
      "\n",
      "These are common questions that interviewers will ask to see if you really understand A/B testing.\n",
      "\n",
      "* Letâ€™s say that you are mid-way through running your A/B test and the performance starts to get worse. It had a strong start but now your success metric is degrading. Why do you think this could be?\n",
      "   * A common answer is novelty effect\n",
      "* Letâ€™s say that your AB test is concluded and your chosen p-value cutoff is 0.05. However, your success metric has a p-value of 0.06. What do you do?\n",
      "   * Some options are: Extend the experiment. Run the experiment again.\n",
      "   * You can also say that you would discuss the risk of a false positive with your business stakeholders. It may be that the treatment doesnâ€™t have much downside, so the company is OK with rolling out the feature, even if there is no true improvement. However, this is a discussion that needs to be had with all relevant stakeholders and as a data scientist or product analyst, you need to help quantify the risk of rolling out a false positive treatment.\n",
      "* Your success metric was stat sig positive, but one of your guardrail metrics was harmed. What do you do?\n",
      "   * Investigate the cause of the guardrail metric dropping. Once the cause is identified, work with the product manager or business stakeholders to update the treatment such that hopefully the guardrail will not be harmed, and run the experiment again.\n",
      "   * Alternatively, see if there is a segment of the population where the guardrail metric was not harmed. Release the treatment to only this population segment.\n",
      "* Your success metric ended up being stat sig negative. How would you diagnose this?Â \n",
      "\n",
      "I know this is really long but honestly, most of the steps I listed could be an entire blog post by itself. If you don't understand anything, I encourage you to do some more research about it, or get the book that I linked above (I've read it 3 times through myself). Lastly, don't feel like you need to be an A/B test expert to pass the interview. We hire folks who have no A/B testing experience but can demonstrate framework of designing AB tests such as the one I have just laid out. Good luck!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.868\n",
      "Positive: 0.132\n",
      "Compound: 0.4404\n",
      "Body: what are some good ressources to learn R on a higher lever and to keep up with the new things?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.043\n",
      "Neutral: 0.875\n",
      "Positive: 0.082\n",
      "Compound: 0.7595\n",
      "Body: My team at work regularly get asked to run incrementally/ Cannibalisation analyses on certain products or product lines to understand if they are (net) additive to our portfolio of products or not, and then of course, quantify the impacts.\n",
      "\n",
      "The approach my team has traditionally used has been to model this with **log-log regression** to get the elasticity between sales of one product group and the product/product group in question.\n",
      "\n",
      "We'll often try account for other factors within this regression model, such as count of products in each product line, marketing spend, distribution etc.\n",
      "\n",
      "So we might end up with a model like:\n",
      "\n",
      "Log(sales_lineA) ~ Log(sales_lineB) + #products_lineA + #products_lineB + other factors + seasonality components \n",
      "\n",
      "\n",
      "I'm having difficulties with this approach because the models produced are so unstable, adding/removing additional factors often causes wild fluctuations in coefficients, significance etc. As a result, I don't really have any confidence in the outputs.\n",
      "\n",
      "Is there an established approach for how to deal with this kind of problem? \n",
      "\n",
      "Keen to hear any advice on approaches or areas to read up on!\n",
      "\n",
      "Thanks\n",
      "\n",
      "Negative: 0.055\n",
      "Neutral: 0.945\n",
      "Positive: 0.0\n",
      "Compound: -0.4404\n",
      "Body: * Applicants with experience in:\n",
      "   * Statistical modeling and/or data mining\n",
      "   * Data analysis and database toolsÂ \n",
      "   * Computer programming (especially Python, R, or SAS)\n",
      "   * Communicating and presenting technical work\n",
      "\n",
      "I have an upcoming interview and am scared I really need an internship, any ideas what I could be asked?\n",
      "\n",
      "Negative: 0.056\n",
      "Neutral: 0.74\n",
      "Positive: 0.204\n",
      "Compound: 0.7644\n",
      "Body: Can anyone suggest a beginner friendly Sports Data Science project? \n",
      "\n",
      "Sports that are interesting to me :\n",
      "\n",
      "Soccer , Formula , Fighting sports etc.\n",
      "\n",
      "Maybe something so i can use either Regression or classification.\n",
      "\n",
      "Thanks a lot!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.04\n",
      "Neutral: 0.88\n",
      "Positive: 0.08\n",
      "Compound: 0.8739\n",
      "Body: I would like to share some thoughts Iâ€™ve been having. Iâ€™ve been looking into different industries to understand what they expect from data scientists, and Iâ€™m concerned about how many job descriptions focus solely on machine learning frameworks and model development.\n",
      "\n",
      "I started in the data science field ten years ago, and I remember when exploratory data analysis (EDA) was a critical and challenging deliverable from the \"data guys.\" It began with a business perspective, raising hypotheses about problems, identifying variables that could explain them, and highlighting missing data that wasnâ€™t being tracked yetâ€”valuable input for engineering. We were bringing value to the table right from the first step.\n",
      "\n",
      "Iâ€™m part of the group that believes data scientists should be the business team's best friends. As long as we understand what kind of decision is being made, we can help. Today, data science is often treated as a purely technical function, and Iâ€™m not sure this is the right approach. We shouldnâ€™t just receive tasks in JIRA like we're simply developing features. The business team shouldn't be the ones deciding how and when we create a model, for example. After all, do you go to the doctor and ask for surgery right away?\n",
      "\n",
      "I remember when building models was really hard, and we all agree that, in the future, it could be as simple as a drag-and-drop tool that anyone can use (isnâ€™t it already like that?). Are we satisfied with reducing our job description to just that? To me, a data scientist is someone who helps make decisions. Data is just the type of evidence we use. This means we should emphasize EDA, causal inference, A/B testing, econometrics, operational research, and so on.\n",
      "\n",
      "During some recruitment processes, Iâ€™ve encountered people with a development background who struggle with methodology (from data leakage to selecting the right metrics to evaluate models). On the other hand, Iâ€™ve met people without a development background who have trouble with coding, limiting their ability to scale their impact. The solution Iâ€™ve found is to pair a tech-savvy person with a â€˜true data scientistâ€™ to empower both. I understand weâ€™ll never find someone who excels at everything, but I feel weâ€™re getting worse in this regard.\n",
      "\n",
      "Negative: 0.032\n",
      "Neutral: 0.962\n",
      "Positive: 0.006\n",
      "Compound: -0.5789\n",
      "Body: I am job searching to leave my current company as I feel the culture is churn and burn and I cannot keep up with my work load in the sales operations analyst role. I am able to process claims, orders, and monthly reports. However, I keep getting assigned work last minute with short turnaround times. My last assignment was 20 min last week to pull data, and each last minute report has a deadline that usually spans from 1-6 hours. I originally came aboard as a first year analyst, but unfortunately it has become expected I should have been up and running 6 months in a complex foodservice company. I asked for more training on understanding what last minute tasks could come up, but the response was that it would be hard to train since anything could come up. More last minute tasks are expected my way soon, and I'm already working 30min to 1 hr late of the 8-5. Should I keep pursuing a new job and is this a realistic/unrealistic expection from the company?\n",
      "\n",
      "Negative: 0.054\n",
      "Neutral: 0.875\n",
      "Positive: 0.071\n",
      "Compound: 0.0973\n",
      "Body: I'm early career, and I honestly still feel like I don't truly know that much about how professional organizations manage their data, their processes for modeling, survey research, cleaning. I feel confident in doing that work on my own, managing my own stuff and obviously what I've learned in school, but I'm not really sure how much professional data scientists and analysts should know when coming out of school and what you actually learn on the job.\n",
      "\n",
      "Negative: 0.03\n",
      "Neutral: 0.893\n",
      "Positive: 0.077\n",
      "Compound: 0.9195\n",
      "Body: Hey - this is for work. \n",
      "\n",
      "20 years into my DS career ... I am being asked to tackle a geospatial problem. In short - I need to organize data with lat long and then based on \"nearby points\" make recommendations (in v1 likely simple averages). \n",
      "\n",
      "The kicker is that I have multiple data points per geo-point, and about 1M geo-points. So I am worried about calculating this efficiently. (v1 will be hourly data for each point, so 24M rows (and then I'll be adding even more)\n",
      "\n",
      "**What advice do you have about best approaching this? And at this scale?**   \n",
      "  \n",
      "Where I am after a few days of looking around   \n",
      "- calculate KDtree\n",
      "- Possibly segment this tree where possible (e.g. by region)  \n",
      "- get nearest neighbors  \n",
      "  \n",
      "I am not sure whether this is still the best, or just the easiest to find because it's the classic (if outmoded) option. Can I get this done on data my size? Can KDTree scale into multidimensional \"distance\" tress (add features beyond geo distance itself)?   \n",
      "  \n",
      "If doing KDTrees - where should I do the compute? I can delegate to Snowflake/SQL or take it to Python. In python I see scipy and SKLearn  has packages for it (anyone else?) - any major differences? Is one way way faster?   \n",
      "  \n",
      "Many thanks DS Sisters and Brothers...\n",
      "\n",
      "Negative: 0.013\n",
      "Neutral: 0.91\n",
      "Positive: 0.077\n",
      "Compound: 0.6254\n",
      "Body: I have a dataset of paragraphs with multiple phrases and the main objective of this project is to do sentiment analysis on the full paragraph + finding phrases that can considered high impact/highlights in the paragraph - sentences that contribute a lot to the final prediction. To do so our training set is the full paragraphs + paragraphs up to a randomly sampled sentence. This on a single model.\n",
      "\n",
      "One thing weâ€™ve tried is predicting the probability of the whole paragraph up to the previous sentence and predicting the probability up to the sentence being evaluated and if the absolute difference in probabilities is above a certain threshold then we consider it a highlight, but after annotating data we came to the conclusion that it does not work very well for our use case because often the highlighted sentences donâ€™t make sense.\n",
      "\n",
      "How else would you approach this issue? I think that this doesnâ€™t work well because the model might already predict the next sentence and large probability changes happen when the next sentence is different from what was â€œpredictedâ€, which often isnâ€™t a highlightâ€¦\n",
      "\n",
      "Negative: 0.021\n",
      "Neutral: 0.837\n",
      "Positive: 0.142\n",
      "Compound: 0.9577\n",
      "Body: Like many data scientists, my gateway drug was genetic algorithms. They're simple and feel incredibly powerful. I remember I solved a toy scheduling problem using a GA in college, and I was floored by how crazy it was that I could find a good schedule, in a few milliseconds, when the solution space contained more possible schedules than there are atoms in the known universe, by making schedules essentially have sex with each other. Wild.\n",
      "\n",
      "Now that I'm writing about AI I've been wanting to explore the topic in one of my articles. However, one of the prerequisites of a topic is that there's a compelling use for whatever I'm talking about, and I am not aware of a \"great resounding din\" for GAs.\n",
      "\n",
      "I would love to write about GAs, but I need a few use cases that are fascinating, actually useful, and are preferably state of the art. I figured I might ask here!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 1.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: For people using ML platform such as sagemaker, azure ML do you still code ?\n",
      "\n",
      "Negative: 0.021\n",
      "Neutral: 0.824\n",
      "Positive: 0.155\n",
      "Compound: 0.9571\n",
      "Body: The last Europe-centric salary thread led to very interesting discussions and insights. So, I'll start another one for 2024:\n",
      "\n",
      "[https://www.reddit.com/r/datascience/comments/17sppgb/europe\\_salary\\_thread\\_whats\\_your\\_role\\_and\\_salary/](https://www.reddit.com/r/datascience/comments/17sppgb/europe_salary_thread_whats_your_role_and_salary/)\n",
      "\n",
      "I think it's worthwhile to learn from one another and see what different flavours of data scientists, analysts and engineers are out there in the wild. In my opinion, this is especially useful for the beginners and transitioners among us. So, do feel free to talk a bit about your work if you can and want to. ðŸ™‚\n",
      "\n",
      "While not the focus, non-Europeans are of course welcome, too. Happy to hear from you!\n",
      "\n",
      "# \n",
      "\n",
      "**Data Science Flavour:** .\n",
      "\n",
      "**Location:** .\n",
      "\n",
      "**Title:** .\n",
      "\n",
      "**Compensation (gross):** .\n",
      "\n",
      "**Education level:** .\n",
      "\n",
      "**Experience:** .\n",
      "\n",
      "**Industry/vertical:** .\n",
      "\n",
      "**Company size:** .\n",
      "\n",
      "**Majority of time spent using (tools):** .\n",
      "\n",
      "**Majority of time spent doing (role):** .\n",
      "\n",
      "Negative: 0.039\n",
      "Neutral: 0.904\n",
      "Positive: 0.057\n",
      "Compound: 0.5396\n",
      "Body: I want to share a new Python library we built that lets you write code on a low-powered laptop and run the code on servers in the cloud.\n",
      "\n",
      "# How does it work?\n",
      "\n",
      "When you run a notebook cell, the code executes on another machine in the cloud instead of your laptop.Â \n",
      "\n",
      "The logs from the remote machine get streamed back to your notebook. It feels like the code is still running in your local notebook, but itâ€™s actually running on a server in the cloud.\n",
      "\n",
      "https://preview.redd.it/yigo85gqkdtd1.png?width=790&format=png&auto=webp&s=175d9bb2e47798b978acea6d2be99743449178fd\n",
      "\n",
      "# BenefitsÂ \n",
      "\n",
      "**You can develop on the cloud without using a cloud notebook.**Â \n",
      "\n",
      "If youâ€™ve ever used a cloud notebook, youâ€™ve probably had your cloud notebook crash and lost your work.Â \n",
      "\n",
      "This lets you develop on a local, low-powered system, while streaming the computation to the cloud.\n",
      "\n",
      "**Local files automatically sync with the cloud runtime**Â \n",
      "\n",
      "You can use files from your local machine in your remote function executions. No need to upload and download weights from Google drive or S3.Â \n",
      "\n",
      "**You can mix-and-match compute across cells**Â Â \n",
      "\n",
      "Does your training code need the same hardware as your inference code? Probably not. This lets you customize the hardware used in your notebook, function-by-function.Â \n",
      "\n",
      "Weâ€™d be happy if you gave this a try! Let us know if you have any feature ideas or suggestions.Â \n",
      "\n",
      "**Website**: [https://beam.cloud](https://beam.cloud)\n",
      "\n",
      "**Example Notebook:** [https://github.com/beam-cloud/examples/blob/main/jupyter\\_notebooks/beam-notebook.ipynb](https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb)\n",
      "\n",
      "**Docs**: [https://docs.beam.cloud/v2/environment/jupyter-notebook](https://docs.beam.cloud/v2/environment/jupyter-notebook)\n",
      "\n",
      "Negative: 0.031\n",
      "Neutral: 0.842\n",
      "Positive: 0.128\n",
      "Compound: 0.9877\n",
      "Body: Iâ€™m in a weird spot where Iâ€™m doing more soft skills than coding right now. Weirdly enough, I donâ€™t really miss the coding. Iâ€™m â€œsellingâ€ my LLMs to business partners who donâ€™t get DS, leadership for funding, etcâ€¦ Iâ€™ve had to do this for every model Iâ€™ve built so far, but this time, I donâ€™t want to both create the models and then sell them. Or sell the idea, create a POC, continue talks/improving the model, go through legal/compliance. Because of the soft skills, Iâ€™ve been able to productionize the most models in my department, but itâ€™s exhausting to do both. I should also add that my manager isnâ€™t even a DS and doesnâ€™t know what theyâ€™re doing, so Iâ€™ve had to go out to business partners from the start to get projects. Legitimately have been doing their job and mine for a while. \n",
      "\n",
      "Iâ€™ve been applying to DS/ML roles to get practice with interviewing and realizing that I really donâ€™t want to go through coding interviews or read research papers anymore. Iâ€™ve been really enjoying business books and other books on soft skills. Iâ€™m also am\n",
      "an extroverted person by nature, so itâ€™s hard for me to go back from talking to coding. I love to present, too and love public speaking. \n",
      "I love to learn and discuss what I learned, I just donâ€™t think I want to build models from scratch or maintain existing models anymore. \n",
      "\n",
      "I love NLP/AI, so Iâ€™d want to stick into that area if I were to go to sales. I know tech sales that are in cloud, and I have a few AWS certs, but have never really heard of sales in data science \n",
      "\n",
      "Negative: 0.069\n",
      "Neutral: 0.899\n",
      "Positive: 0.032\n",
      "Compound: -0.3527\n",
      "Body: How much DSA can be expected in online assessment and interview of a DS role? Is it similar to SDE roles now? I want to know about the difficulty level. Particularly for product based companies or big tech.\n",
      "\n",
      "Negative: 0.014\n",
      "Neutral: 0.897\n",
      "Positive: 0.089\n",
      "Compound: 0.7346\n",
      "Body: Hi guys,\n",
      "\n",
      "I have been researching a lot over which one to choose. While there is substantial evidence, Claude seems superior for coding, the message limits seems to vary rendering it slightly ineffective. Whereas ChatGPT seems to give similar results with slightly more limits. It also allows more than text media as well.\n",
      "\n",
      "My main purposes will be regards to data science based coding and job hunt tasks ( proofreading, customizing resumes etc. )\n",
      "\n",
      "What would you have chosen?\n",
      "\n",
      "\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 1.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: Check out this job at CONNECTMETA.AI:  https://www.linkedin.com/jobs/view/4041564585\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 1.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: thoughts on this? I get the point she is making but find myself weirded out with all this gatekeeping \"real science\"\n",
      "\n",
      "[https://youtu.be/ALPOwJlOj1k?si=qy8BVuhWuIZPZKoA](https://youtu.be/ALPOwJlOj1k?si=qy8BVuhWuIZPZKoA)  \n",
      "\n",
      "\n",
      "Negative: 0.04\n",
      "Neutral: 0.882\n",
      "Positive: 0.079\n",
      "Compound: 0.8713\n",
      "Body: The company I'm interviewing with sent me a take-home technical assessment instead of a technical interview. I've never had a take-home before, so I didn't know exactly what to expect, but right away it seemed too much. I know it's an employer's market right now, but even so, the assignment looked too long. There were several questions and each would take me a few hours to work through, so we're talking days here. \n",
      "\n",
      "Then I thought, maybe the assignment is not that long and I'm just not a very good data scientist. So I pasted the assignment to ChatGPT and asked how long it would take an average data scientist to complete this. The answer was 32 hours, give or take. That's 4 full working days dedicated to this assignment.\n",
      "\n",
      "That's insane, right? Even in this market. I understand it's only an estimate (a lot of variables to consider) but it sounds about right to me. It would take me about that long to complete this assignment. \n",
      "\n",
      "Thankfully I have another offer on the table, so I feel absolutely no pressure to do this. It's actually kind of funny as I'm contemplating what to do next. Part of me is tempted to do it just to see where this goes. Or just give them ChatGPT answers for every question and see if it gets me anywhere, lol. \n",
      "\n",
      "Then another part of me wants to send a strongly worded email on why this is a terrible way to recruit quality candidates. \n",
      "\n",
      "What would you do in this situation?\n",
      "\n",
      "Negative: 0.026\n",
      "Neutral: 0.901\n",
      "Positive: 0.073\n",
      "Compound: 0.9604\n",
      "Body: I work in an organisation with medical researchers who work on medical datasets and clinical trials. Previously we had a custom made tool with a simple GUI to select and filter data, and it could also do (limited) grouping and pivoting of data. However the datasets have outgrown the scope of our tool, so currently we just provide the datasets as SQL databases, together with basic SPSS and Rstudio scripts to select all data and add metadata (labels, codes+labels etc). A typical required workflow for our reseach users is like this:\n",
      "\n",
      "* open 3 datasets, for example patient treatments, medicine-use, labresults\n",
      "* filter the patient treatments on a specific treatment\n",
      "* join with medicine-use\n",
      "* separately join patient treatments with labresults with patients\n",
      "* filter labresults on certain types, for example Ht, Hb, Crea\n",
      "* filter labresults on single result closest to patient treatments date\n",
      "* pivot labresults from \"long format\" to \"wide format\" (i.e. one column per resultvalue of type Ht/Hb/Crea)\n",
      "\n",
      "Part of our problem is that the data and queries are almost always different, due to the ad hoc nature of research, so we can't provide standard scripts. The example is labresults and medicine, but other times it could be medicalprocedures. questionnaires, height/weight, ECG data, lung spirometer, eye exams etc etc anything really, you name it.\n",
      "\n",
      "Also there's high turnover among our users because of PhD students, short project contracts, parttime researchers etc. so while they do have some training with SPSS and Rstudio they're mostly non-technical users with limited coding skills.\n",
      "\n",
      "All this to say; getting our users to write basic filter and join-scripts takes quite some effort, let alone the more \"advanced\" stuff like grouping or row-to-column pivoting. So I was wondering:\n",
      "\n",
      "* Is there a tool or app that supports the workflow above in an intuitive/visual way? So not just writing code in R-studio or SPSS.\n",
      "\n",
      "To be clear, I don't mean a data visualization tool for creating graphs. I mean like maybe the datasets are displayed as spreadsheet and then users do the filtering and joining using easy-to-use widgets. Or maybe all the operations are put together using visual blocks, similar to [Scratch programming](https://www.google.com/search?q=programming+scratch). Ideally it would also still output a script which does the same steps (in R-script or SPSS or whatever) so that the steps are reproducible.\n",
      "\n",
      "Negative: 0.034\n",
      "Neutral: 0.82\n",
      "Positive: 0.146\n",
      "Compound: 0.7992\n",
      "Body: I'm fooling around building a grammer of graphics implementation in python using plotly as a backend. I know that Plotnine exists but it isn't interactive, and of lets-plot, but I don't think its compatible with many dashboarding frameworks. If anyone wants to help out, feel free.\n",
      "\n",
      "[bbcho/ggplotly (github.com)](https://github.com/bbcho/ggplotly)\n",
      "\n",
      "Negative: 0.051\n",
      "Neutral: 0.704\n",
      "Positive: 0.246\n",
      "Compound: 0.9928\n",
      "Body: First of all tysm y'all for helping me with karma and here I go with my query!\n",
      "\n",
      "I can't attend the meet up and all those networking events I know I do not sound a perfect person for the job but I can assure you I'm pretty good with my work and about networking I can grow it not in person currently as I'm pretty much messed up right now ... So how do I find recommendations online for jobs , or how to grow network online or how to grow get into those good connections ??? \n",
      "\n",
      "P.s I have been trying linkedin but haven't received good response from thr yet should I get a premium or do something else differently to finally build those healthy and helpful connections?? \n",
      "\n",
      "Please tell me I wish to apply for jobs but pretty lost right now and not able to really find anything *sigh** \n",
      "\n",
      "Has anyone grown their network online, it would be great if you could share your process and experience too... \n",
      "\n",
      "Thanks in advance :â -â )\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.046\n",
      "Neutral: 0.908\n",
      "Positive: 0.046\n",
      "Compound: 0.1309\n",
      "Body: Now that I have your attention, let me give a bit of context.\n",
      "\n",
      "My team is responsible for validating a data migration. The schemas in the source and target systems are different and there are some other complicating factors, so the whole project is quite intricate. Team member 1 (T1) was responsible for writing a script to carry out part of this validation automatically. They were writing this script based on consultations with the data engineers and software engineers working on the migration.\n",
      "\n",
      "\n",
      "Then the department head announced a big reorganization. T1 would be moved to another team under the same department, while two people from another team (T2 and T3) would join my team. T1 said they would finish their script before leaving the team, and train myself, T2, and T3 on how to run it and interpret the results.\n",
      "\n",
      "\n",
      "However, things did not go so smoothly. As we began the training sessions, T1 told us that they hadn't been able to finish the script in time, and that they would explain to us how it works and how to finish it. T1 would do some parts, while T2, T3, and myself would handle other parts.\n",
      "\n",
      "\n",
      "The first issue was that it was very difficult to follow the training sessions. T1 is just not good at explaining things. They are verbose and unclear, and so is their documentation. Their code is also very difficult to follow. The other issue is that a lot of the details of the migration only exist in T1's head. Their justification for this or that is often \"well I had a conversation with this engineer about it.\" So it's hard to ascertain the reasoning behind many parts of the script, which then makes it impossible to finish it. T2, T3, and other colleagues have agreed with me on these points.\n",
      "\n",
      "\n",
      "As the training sessions continued day after day, T1 would get increasingly snippy and passive aggressive with us when we asked questions. Put simply: it was not a positive learning environment.\n",
      "\n",
      "\n",
      "Things really came to a head on Friday though. T1 has an unusual approach to developing their script. T1 keeps a master copy of the script on several tabs in a Google Sheet. When part of the script needs to be changed, T1 copies that part out into a Google Doc interspersed with instructions (the instructions aren't code or comments). Then T1 reviews the Google Docs, tests the code from them, copies chunk by chunk around the instructions, and pastes them back into the Google Sheet. T1 was having us follow this method to finish the script.\n",
      "\n",
      "\n",
      "I think this approach is absolutely nuts. It's not reasonable to have 4 people working on a program without some form of version control in place, and Google Docs/Sheets are not good places for writing code. I copied the code from the Sheet into a GitHub repo and added T2 and T3 to it. \n",
      "\n",
      "\n",
      "I reached out to T1 and explained my position. T1 asked to talk to T2 and I on the phone. T1's view is that source control isn't appropriate for creating new code, only for maintaining existing code, and that it would only slow us down. \"I know what source control is. Check in, check out... yeah that's going to take forever.\"\n",
      "\n",
      "\n",
      "T1 also doesn't see an issue with coding in Google Docs/Sheets. I disagreed. T1 then got super passive aggressive and basically said they were going to stop helping to finish the script completely and focus on their new job.\n",
      "\n",
      "\n",
      "I brought this up with my manager and explained everything. They agreed with me and are escalating to the department head. At this point I really don't want to work with T1 anymore. I would rather they finish the script on their own, or me, T2, and T3 do it on our own. The issue with the latter option is that the code is so difficult to follow, and so much of the knowledge to finish it only exists in T1's head, that I think we would have to start from scratch.\n",
      "\n",
      "\n",
      "I realize this is a very long post, so thanks for reading this far. Has anyone here dealt with a similar situation and have any advice?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.94\n",
      "Positive: 0.06\n",
      "Compound: 0.8807\n",
      "Body: Context: I'm working with a team that has extensive experience with causal modeling, but now is working on a project focused on predicting/forecasting outcomes for future events. I've worked extensively on various forecasting and prediction projects, and I've noticed that several people seem to approach prediction with a causal modeling mindset.\n",
      "\n",
      "Example: Weather impacts the outcomes we are trying to predict, but we need to predict several days ahead, so of course we don't know what the actual weather during the event will be. So what someone has done is create a model that is using historical weather data (actual, not forecasts) for training, but then when it comes to inference/prediction time, use the n-day ahead weather forecast as a substitute. I've tried to explain that it would make more sense to use historical weather forecast data, which we also have, to train the model as well, but have received pushback (\"it's the actual weather that impacts our events, not the forecasts\").\n",
      "\n",
      "How do I convince them that they need to think differently about predictive modeling than they are used to?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.942\n",
      "Positive: 0.058\n",
      "Compound: 0.0258\n",
      "Body: As the title suggests, what resources do you suggest to learn recommender systems ML to reach an intermediate-like level \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.912\n",
      "Positive: 0.088\n",
      "Compound: 0.4003\n",
      "Body: Excited to release ryp, a Python package for running R code inside Python! ryp makes it a breeze to use R packages in your Python data science projects.\n",
      "\n",
      "https://github.com/Wainberg/ryp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.085\n",
      "Neutral: 0.807\n",
      "Positive: 0.109\n",
      "Compound: 0.8999\n",
      "Body: In my experience, when people consider applying LLMs to a project they often fall into two camps:\n",
      "\n",
      "1. they turn the project into a chat bot\n",
      "2. they use an LLM for some key feature in a larger application, resulting in an error prone mess\n",
      "\n",
      "there's tremendous power in using LLMs to power specific features within larger applications, but LLMs inconsistency in output structure makes it difficult to use their output within a programmatic system. You might ask an llm to output JSON data, for instance, and the LLM decides it's appropriate to wrap the data in a `\\`\\`\\`json \\`\\`\\`` markdown format. you might ask an LLM to output a list of values, and it responds with something like this:\n",
      "\n",
      "    here's your list\n",
      "    [1,2,3,4]\n",
      "\n",
      "There's an infinite number of ways LLM output can go wrong, which is why output parsing is a thing.\n",
      "\n",
      "I've had the best luck, personally, with LangChain in this regard. [LangChain's pydantic parser](https://python.langchain.com/docs/how_to/output_parser_structured/) allows one to define an object which is either constructed from the LLMs output, or an error gets thrown. They essentially use a clever prompting system paired with the user's defined structure to coax the model into a consistent output.\n",
      "\n",
      "That's not fool proof either, which is why it's a common practice to either re-try or re-prompt. You can either just re-prompt on a failure, or pass the response which failed to parse to the LLM again and ask the LLM to correct it's mistake. For robust LLMs this works consistently enough where it's actually viable in applications (assuming proper error handling). I made a post about [LangGraph](https://www.reddit.com/r/datascience/comments/1fknbyi/langgraph_allows_you_to_make_falsifiable_testable/) recently, this can also be used to construct complex loops/decisions which can be useful for adding a level of robustness into LLM responses.\n",
      "\n",
      "If you can learn how to consistently turn an LLMs output into JSON, there's a whole world of possible applications.\n",
      "\n",
      "I'm curious what LLM parsing tricks you employ, and what you've seen the most success with!\n",
      "\n",
      "Negative: 0.044\n",
      "Neutral: 0.927\n",
      "Positive: 0.028\n",
      "Compound: -0.25\n",
      "Body: I'm having some difficulty with a sales forecasting project and need some help.\n",
      "\n",
      "**Dataset:** Weekly sales data; So columns such as Store, Item, Week of Year, Sales. This is the most minimal part of the dataset. I can pull in some features such as store dimensional info, item dimensional info, price, and if it is on sale. The date range is about 150 weeks. About 10 unique items and 1000 unique stores.\n",
      "\n",
      "**Objective:** Forecast 1 week out.\n",
      "\n",
      "My accuracy metric, is 1 - ( sum of absolute errors / sum of actual sales ). I need to achieve an accuracy of at least 0.75.\n",
      "\n",
      "**What I have tried:** ARIMA, ETS, xgboost and lightgbm. However, with all these models, I can only achieve an accuracy of 0.35 (with lightgbm). With the ML models, I have tried using tweedie objective, and used a plethora of lagged and rolling features. Most of my data are 0's, and if they are not 0's, tend to be smaller numbers (< 10). Making it hard to accurately forecast.\n",
      "\n",
      "I'm at my wits end and would appreciate any advice.\n",
      "\n",
      "Negative: 0.027\n",
      "Neutral: 0.87\n",
      "Positive: 0.103\n",
      "Compound: 0.9413\n",
      "Body: \n",
      "\n",
      "\n",
      "Hello, after my masters in stats I took up a job in data science. While itâ€™s been fun working and the work is really interesting, part of me still craves keeping up with the stuff I learned in school. I currently do this by reading topics in statistics I never learned in school to keep my knowledge base wide, and revise old topics if need be (sometimes they come up in work).\n",
      "\n",
      "But I feel if I was able to teach this material to someone, Iâ€™d be able to keep myself accountable to know it deeply. Like, yes I know the theory of the linear model reasonably well or I know hypothesis testing or time series well, but if I had to teach this to someone, I feel as though Iâ€™d be able to actually make sure I retain it for long term memory, because itâ€™s not always where Iâ€™m actually thinking about this stuff at work.\n",
      "\n",
      "One of the ways I thought of was volunteering to teach math to students. I donâ€™t know how Iâ€™d do this but I want a way to actually volunteer my time to do this, whether it be for some kind of cause, or just for someone whoâ€™s learning it. Also a way to kill time on the weekends. \n",
      "\n",
      "Anyone know of good ways to do this ?\n",
      "\n",
      "Negative: 0.05\n",
      "Neutral: 0.758\n",
      "Positive: 0.192\n",
      "Compound: 0.9936\n",
      "Body: Hi everyone,\n",
      "\n",
      "Iâ€™m currently working as a Senior Data Scientist in Germany. I hold a PhD in Physics with a very high GPA, have completed all the relevant Coursera courses, and Iâ€™m in my mid-30s.\n",
      "\n",
      "So far, things have been going well, but my job mainly involves visualizing data in Tableau and writing lengthy SQL queries. Recently, Iâ€™ve been lucky to work on some GenAI projects, but that's still new territory for me.\n",
      "\n",
      "I initially took this job because I was going through a tough time and needed an \"easy\" role. However, Iâ€™m now eager to change my job and take on more challenging opportunities. In my region, interesting job positions only become available every few months at most, which makes the search even more competitive and frustrating.\n",
      "\n",
      "When applying for new positions, I sometimes get invited to interviews for high-skill roles that seem like a good fit. However, I struggle to talk about exciting achievements from my last three years. The GenAI/NLP projects Iâ€™ve been involved in are quite recent (only about three months), and our team is limited by resourcesâ€”small GPU, not enough dataâ€”so we canâ€™t do things like training LoRA adapters for different use cases.\n",
      "\n",
      "I feel stuck in underwhelming roles, and high-skill positions feel out of reach, even though I believe I could contribute effectively.\n",
      "\n",
      "Additionally, I often find myself being too honest during interviews. When asked questions like what percentage of my daily job involves coding or about my expertise in NLP, I tend to share the full truth, highlighting my limitations.\n",
      "\n",
      "**Has anyone experienced something similar or have tips on how to better present my skills and experiences during interviews without underselling myself?**\n",
      "\n",
      "Thanks in advance!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.781\n",
      "Positive: 0.219\n",
      "Compound: 0.8126\n",
      "Body: basically the title. I want to know what the best universities are in the US which offer masters in data science, after which I can get into a good product data science role\n",
      "\n",
      "Negative: 0.196\n",
      "Neutral: 0.732\n",
      "Positive: 0.072\n",
      "Compound: -0.765\n",
      "Body: I used to be able to tell if I failed an interview but now it seems even good questions and feedback and talking about the next steps just comes with rejections \n",
      "\n",
      "I don't get if the market has changed or I got worse. \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 1.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: Have any of you gone from Data Scientist to Data Analyst? If so, how'd you handle the interviews asking why you're \"going back to analyst work\" after building models, running experiments, etc.?\n",
      "\n",
      "Negative: 0.069\n",
      "Neutral: 0.877\n",
      "Positive: 0.054\n",
      "Compound: -0.1838\n",
      "Body: Received a take-home exercise and am completely bored out of it. \n",
      "They didn't even ask 'is now a good time', just sent a link and needs to be done in a week.\n",
      "\n",
      "The type that says here is a gig of random data, with nested fields everywhere, and no clear ask.\n",
      "\n",
      "I kind of spend most of the time ranting to myself that i shouldn't take this sort of sh*t, have better things to do that sort out the schema of some random company, and realizing how much over the years i've started to dislike the standard wrangling with pandas.\n",
      " \n",
      "The only problem is that I currently desperately need a job, this is the only sort of gigs I hear back from, and reading the posts here I should be even happy to get any reply.\n",
      "\n",
      "Anyway,to conclude this rant with a question..how much time do you guys actually put in on these sorts of tortures. It seems just a clear case of more time, better result, but we got to draw a line somewhere right?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negative: 0.079\n",
      "Neutral: 0.828\n",
      "Positive: 0.093\n",
      "Compound: -0.4107\n",
      "Body: Had my 3rd round interview today which was a technical based. I guess it went â€¦ bad. It was with the vp of the company. It seemed like he had already made up his mind right at the beginning and felt like i was at an uphil battle. He didnt even know if I had any interviews before this and I told him i spoke to guy1(principal data engineer) and guy2(senior data engineer) Been working as data analyst for past 3 years and this is a data analyst/engineer position at a startup(which is quite big now) and the role is amazing int terms of growth opportunity, pay, culture, every aspect and I can thrive in it too imo.\n",
      "\n",
      "He asked me about my resume then asked what is categorical data. I said in a diff tables categorised for diff information like student tsble, prof table. Then asked was I correct ? He said not quite. Its diff categories of prof tables . He then going thru resume and stuff was like this seems to be a mismatch for the role(it was not!) i said i had discussions eith guy1 and guy2 and role is 80-90% sql which ive been using past few years. He then shared a coderdata link to do a query. He could see what i type, but i couldnt run or test queries. I was trying to talk through my thought process through but he seemed uninterested. I did the query by the end when time ran out and he said i have to hop off. but whole time there was *less than smooth* communication. It was so frustrating.\n",
      "\n",
      "Im thinking to reach out to recruiter and share my experience and if any possibility of another attempt. I dont have much hopes but might as well. This is disheartening as I shouldve been able to clear this smoothly but I was so forward to looking progress but its depressing bcs market is already so competitive and brutal. After like 500+ applications I got like 1-2 interviews and I managed to get to 3rd round only for this to happen *sigh*. Ultimately he has the final say since hes vp despite having good conversations with principal data engineer, senior data engineer in previous interviews :(\n",
      "\n",
      "Negative: 0.036\n",
      "Neutral: 0.894\n",
      "Positive: 0.07\n",
      "Compound: 0.3899\n",
      "Body: I did my undergrad and Msc in data science, now going to the industry I feel I might lose touch with some topics and techniques. I was thinking about starting a series on medium where I deep dive into different topics in the field. It would get me to study, be updated and get more visibility, what do you think? Will this be good for me? Is this something worth pursuing?\n",
      "\n",
      "Negative: 0.058\n",
      "Neutral: 0.806\n",
      "Positive: 0.137\n",
      "Compound: 0.932\n",
      "Body: I am working on a project and looking for some help from the community. The project's goal is to find any kind of relationship between MetricA (integer data eg: Number of incidents) and 5-10 survey questions. The survey question's values are from 1-10. Being a survey question, we can imagine this being sparse. There are lot of surveys with no answer. \n",
      "\n",
      "I have grouped the data by date and merged them together. I chose to find the average survey score for each question to group by. This may not be the greatest approach but this I started off with this and calculated correlation between MetricA and averaged survey scores. Correlation was pretty weak. \n",
      "\n",
      "Another approach was to use xgboost to predict and use shap values to see if high or low values of survey can explain the relationship on predicted MetricA counts. \n",
      "\n",
      "Has any of you worked anything like this? Any guidance would be appreciated! \n",
      "\n",
      "Negative: 0.052\n",
      "Neutral: 0.82\n",
      "Positive: 0.128\n",
      "Compound: 0.5987\n",
      "Body: I know that some (most?) recruiters and HMs don't look at your github. But for those who do, what do you want to see in there? What impresses you the most? \n",
      "\n",
      "Is there anything you do NOT like to see on GH? Any red flags?\n",
      "\n",
      "Negative: 0.016\n",
      "Neutral: 0.904\n",
      "Positive: 0.08\n",
      "Compound: 0.5885\n",
      "Body: Hi all,\n",
      "\n",
      "I've been working with a client and they needed a way to display inline PDFs in a Dash app. I couldn't find any solution so I built one: [dash-pdf](https://github.com/ploomber/dash-pdf)\n",
      "\n",
      "It allows you to display an inline PDF document along with the current page number and previous/next buttons. Pretty useful if you're generating PDFs programmatically or to preview user uploads.\n",
      "\n",
      "It's pretty basic since I wanted to get something working quickly for my client but let me know if you have any feedback of feature requests.\n",
      "\n",
      "https://i.redd.it/mxznjgrwt8sd1.gif\n",
      "\n",
      "\n",
      "\n",
      "Negative: 0.04\n",
      "Neutral: 0.831\n",
      "Positive: 0.129\n",
      "Compound: 0.743\n",
      "Body: Currently a 4th year data science undergrad who already has two internships and currently doing a capstone project/contract work with a company. I have the opportunity to do undergrad research as well but kind've burnt out at the moment and feel like my resume is \"good enough\" and should maybe just focus on job interviews. Am I just being lazy or should I do the undergrad research for grad school applications/letters of rec.\n",
      "\n",
      "Negative: 0.007\n",
      "Neutral: 0.819\n",
      "Positive: 0.174\n",
      "Compound: 0.9899\n",
      "Body: Hi all, I currently work for a company as somewhere between a data analyst and a data scientist. I have recently been tasked with trying to create a model/algorithm to help classify our help deskâ€™s chat data. The goal is to be able to build a model which can properly identify and label the reason the customer is contacting our help desk (delivery issue, unapproved charge, refund request, etc). This is my first time working on a project like this, I understand the overall steps to be get a copy of a bunch of these chat logs, label the reasoning the customer is reaching out, train a model on the labeled data and then apply it to a test set that was set aside from the training data but Iâ€™m a little fuzzy on specifics. This is supposed to be a learning opportunity for me so itâ€™s okay that I donâ€™t know everything going into it but I was hoping you guys who have more experience could give me some advice about how to get started, if my understanding of the process is off, advice on potential pitfalls, or perhaps most helpful of all any good resources that you feel like helped you learn how to do tasks like this. Any help or advice is greatly appreciate!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.892\n",
      "Positive: 0.108\n",
      "Compound: 0.3939\n",
      "Body: Hey hey, just stumbled upon this [ELL](https://docs.ell.so/index.html) thing and curious if anyone tried it. How does it compare to langchain? Are they complementary?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.896\n",
      "Positive: 0.104\n",
      "Compound: 0.8658\n",
      "Body: I didn't think this market would be able to surprise me with anything, but check this out.\n",
      "\n",
      "# 2025 Data Science Intern\n",
      "\n",
      "at Viking Global InvestorsÂ New York, NY2025 Data Science Intern\n",
      "\n",
      "  \n",
      "*The base salary range for this position in New York City is annual*Â ***$175,000 to $250,000.***Â *In addition to base salary, Viking employees may be eligible for other forms of compensation and benefits, such as a discretionary bonus, 100% coverage of medical and dental premiums, and paid lunches.*\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Found it here: [https://jobs-in-data.com/](https://jobs-in-data.com/)\n",
      "\n",
      "Job offer: [https://boards.greenhouse.io/vikingglobalinvestors/jobs/5318105004](https://boards.greenhouse.io/vikingglobalinvestors/jobs/5318105004)\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.024\n",
      "Neutral: 0.872\n",
      "Positive: 0.105\n",
      "Compound: 0.6343\n",
      "Body: Hello, I will have to open a data science division for internal purpose in my company soon.\n",
      "\n",
      "What do you guys recommend to provide a good start ? We're a small DS team and we don't want to use any US provider as GCP, Azure and AWS (privacy).\n",
      "\n",
      "Negative: 0.04\n",
      "Neutral: 0.889\n",
      "Positive: 0.072\n",
      "Compound: 0.5434\n",
      "Body: Time series forecasting seems to have been the next wave of modeling which had gotten â€œauto-MLedâ€ so to speak in every company. Itâ€™s like, â€œwe have some existing forecasting models we already use, they are good enough, we donâ€™t need a data scientist to go in and build a new time series modelâ€.\n",
      "\n",
      "It seems as though itâ€™s rare to find actual jobs involving building custom time series models in Stan, or like actually trying to think more rigorously about the problem. Is everything just â€œthrow it into prophetâ€ are are there any people here who are actually building custom/bespoke time series models \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.748\n",
      "Positive: 0.252\n",
      "Compound: 0.7745\n",
      "Body: I've been playing with NotebookLM a bit, fed it last weeks top posts and it created a mini summary in the form of a podcast. Turned out not bad!\n",
      "\n",
      "https://soundcloud.com/tree3_dot_gz/r-datascience-1\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.944\n",
      "Positive: 0.056\n",
      "Compound: 0.6696\n",
      "Body:  \n",
      "\n",
      "Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n",
      "\n",
      "* Learning resources (e.g. books, tutorials, videos)\n",
      "* Traditional education (e.g. schools, degrees, electives)\n",
      "* Alternative education (e.g. online courses, bootcamps)\n",
      "* Job search questions (e.g. resumes, applying, career prospects)\n",
      "* Elementary questions (e.g. where to start, what next)\n",
      "\n",
      "While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.952\n",
      "Positive: 0.048\n",
      "Compound: 0.3612\n",
      "Body: Iâ€™m B2C data analyst transitioned to B2B SaaS Product analytics. I feel that some methods used in B2C are not applicable in B2B. I would like to know more about interpreting metrics (retention, expansions/contractions, cohort analysis, etc), and grasping the business side. Not looking for basic stats/ML booksâ€”any practical book recommendations?\n",
      "\n",
      "Negative: 0.136\n",
      "Neutral: 0.821\n",
      "Positive: 0.043\n",
      "Compound: -0.9089\n",
      "Body: Iâ€™ve taken on a lot of additional responsibility without a compensation adjustment. Iâ€™ve just been asked to take on more. How do I professionally say Iâ€™m not going to do that unless I get a raise.\n",
      "\n",
      "I have 15 YOE and never received a raise. I usually just leave when I get told no raise, but actually donâ€™t want to leave this time.\n",
      "\n",
      "  \n",
      "Edit:\n",
      "\n",
      "In summary, I need to:\n",
      "\n",
      "1. Make a compelling case why I deserve the raise (Not sure why triple workload isnâ€™t compelling enough) and/or\n",
      "\n",
      "2. Have an offer and be willing to leave if necessary. The problem here is I am tired of always leaving to get a raise. Spending 6 months of countless interviews just to get counter offer and stay also seems dumb.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.849\n",
      "Positive: 0.151\n",
      "Compound: 0.9757\n",
      "Body: Title. I have a 30 min technical assessment interview followed by 45min \\*discussion/behavioral\\* interview with another person next week for a data analyst position(although during the first interview the principal engineer described the responsibilities as data engineering oriented and i didnt know several tools he mentioned but he said thats ok dont expect you to right now. anyway i did move to second round). the job description is just standard data analyst requirements like sql, python, postgresql, visualization reports, develop/maintain data dictionaries, understanding of data definition and data structure stuff like that. Ive been practicing medium/hard sql queries on leetcode, datalemur, faang interview sql queries etc. but im kinda feeling in the dark as to what should i be ready for. i am going to doing 1-2 eda python projects and brush up on p-bi. I'd really appreciate if any of you can provide some suggestions/tips to help prepare. Thanks.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.873\n",
      "Positive: 0.127\n",
      "Compound: 0.9814\n",
      "Body: Hi, I'm interested in your opinion regarding what is the best infra setup and stack for a small DS team (up to 5 seats). If you also had a ballpark number for the infrastructure costs, it'd be great, but let's say cost is not a constraint if it is within reason.\n",
      "\n",
      "The requirements are:\n",
      "\n",
      "* To store our repos. We can't use Github.\n",
      "* To be able to code in Python and R\n",
      "* To have the capability to access computing power when needed to run the ML models. There are some models we have that can't be run in laptops. At the moment, the heavy workloads are run in a Linux server running RStudio Server, which basically gives us an IDE contained in the server to execute Python or R scripts.\n",
      "* Connect to corporate MS SQL or Azure SQL databases. How a solution with Azure might look like? Do we need to use Snowflake or Datababricks on top of Azure or would Azure ML be enough?\n",
      "* Nice to have: to able to share bussiness apps, such as dashboards, with the business stakeholders.  How would you recommend to deploy these Shiny, streamlit apps? Docker containers using Azure or Posit Connect? How can Alteryx be used to deploy these apps?\n",
      "\n",
      "Which setups do you have at your workplaces? Thank you very much!\n",
      "\n",
      "Negative: 0.014\n",
      "Neutral: 0.918\n",
      "Positive: 0.068\n",
      "Compound: 0.6848\n",
      "Body: Iâ€™ve been thinking on this and havenâ€™t been able to think of a decent solution.\n",
      "\n",
      "Suppose you are trying to forecast demand for items at a grocery store. Maybe you have 10,000 different items all with their own seasonality that have peak sales at different times of the year.\n",
      "\n",
      "Are there any single models that you could use to try and get timeseries forecasts at the product level? Has anyone dealt with similar situations? How did you solve for something like this?\n",
      "\n",
      "Because there are so many different individual products, it doesnâ€™t seem feasible to run individual models for each product.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.891\n",
      "Positive: 0.109\n",
      "Compound: 0.5514\n",
      "Body: Have you used agile in your project management?  How has your experience been? Would you rather do waterfall or hybrid? What benefits of agile do you see for data science?\n",
      "\n",
      "Negative: 0.097\n",
      "Neutral: 0.865\n",
      "Positive: 0.038\n",
      "Compound: -0.9181\n",
      "Body: I recently [conducted a study](https://www.eyelevel.ai/post/do-vector-databases-lose-accuracy-at-scale?utm_source=x&utm_medium=social&utm_id=santiago-rag2) that compared three approaches to RAG across four document sets. These document sets consisted of documents which answered the same questions posed to the RAG systems, but also contained an increasing number of erroneous documents which were not relevant to the questions being asked. We tested 1k, 10k, 50k, and 100k pages and found some RAG systems can be upwards of 10% less performant on the same questions when exposed to an increased quantity of irrelevant pages.\n",
      "\n",
      "Within this study there seemed to be a major disparity in vector search vs more traditional textual search systems. While these results are preliminary, they suggest that vector search is particularly susceptible to a degradation in performance with larger document sets, while search with ngrams, hierarchical search, and other classical strategies seem to experience much less performance degradation.\n",
      "\n",
      "I'm curious about who has used vector vs. traditional text search in RAG. Have you noticed any substantive differences? Have you had any problems with RAG at scale?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.737\n",
      "Positive: 0.263\n",
      "Compound: 0.9393\n",
      "Body: Qwen2.5 by Alibaba is considered the best open-sourced model for coding (released recently) and is a great alternate for Claude 3.5 sonnet. I tried creating a basic car game for web browser using it and the results were great. Check it out here : https://youtu.be/ItBRqd817RE?si=hfUPDzi7Ml06Y-jl\n",
      "\n",
      "Negative: 0.032\n",
      "Neutral: 0.846\n",
      "Positive: 0.122\n",
      "Compound: 0.9901\n",
      "Body: This happened yesterday. I wrote an internal report for my company on the effectiveness of tool use for different large language models using tools we commonly utilize. I created a challenging set of questions to benchmark them and measured accuracy, latency, and cost. I sent these insights to our infrastructure teams to give them a heads up, but I also posted in a LLM support channel with a summary of my findings and linked the paper to show them my results.\n",
      "\n",
      "A lot of people thanked me for the report and said this was great informationâ€¦ but one guy, who looked like he was in his 50s or 60s even, started going off about how I needed to learn Python and write my own functionsâ€¦ despite the fact that I gave everyone access to my repo â€¦ that was written in Python lol. His takeaway was also thatâ€¦ we should never use tools and instead just write our own functions and ask the model which tool to useâ€¦ which is basically the same thing. He clearly didnâ€™t read the 6 page report I posted. I responded as nicely as I could that while some models had worse accuracy than others, I didnâ€™t think the data indicated we should abandon tool usage. I also tried to explain that tool use != agents, and thought maybe that was his point?\n",
      "\n",
      "I explained again this was a benchmark, but he â€¦ just could not understand the concept and kept trying to offer me help on how to change my prompting and how he had tons of experience with different customers. I kept trying to explain, Iâ€™m not struggling with a use case, Iâ€™m trying to benchmark a capability. I even tried to say, if you think your approach is better, document it and test it. To which he responded, Iâ€™m a practitioner, and talked about his experience againâ€¦ after which I just gave up.\n",
      "\n",
      "Anyway, not sure there is a point to this, just wanted to rant about people confidently giving you adviceâ€¦ while not actually reading what you wrote lol.\n",
      "\n",
      "Edit: while I didnâ€™t do it consciously, apologies to anyone if this came off as ageist in any way. Was not my intention, the guy just happened to be older.\n",
      "\n",
      "Negative: 0.018\n",
      "Neutral: 0.824\n",
      "Positive: 0.158\n",
      "Compound: 0.9518\n",
      "Body: A team I am working in has been approved to become the a new data science organization to support the broader team as a whole. We have 3-5 technical(our team) and about 20 non-technical individuals that will have asks for us. Are there any good resources for how to build this organization from scratch with frameworks for approaches to asks, team structure, best practices, etc. TIA!\n",
      "\n",
      "Edit: Not hiring anyone new. Please stop messaging me about that. \n",
      "\n",
      "Edit 2: mostly looking for resources related to workflow integration within a larger department. How can they have their ideas come to us, we yea/nay them, backlog refinement from there\n",
      "\n",
      "Negative: 0.007\n",
      "Neutral: 0.894\n",
      "Positive: 0.099\n",
      "Compound: 0.98\n",
      "Body: I know this question hast been asked a lot and you are probably annoyed by it. But what is the best way of keeping Miniforge up to date?\n",
      "\n",
      "The command I read mostly nowadays is:\n",
      "```\n",
      "mamba update --all\n",
      "```\n",
      "\n",
      "But there is also:\n",
      "```\n",
      "mamba update mamba\n",
      "mamba update --all\n",
      "```\n",
      "\n",
      "Earlier there was:\n",
      "```\n",
      "(conda update conda)\n",
      "conda update --all)\n",
      "```\n",
      "\n",
      "1. I guess the outcome of the `conda` command would be equivalent to the `mamba` command, am I correct?\n",
      "2. But what is the use of updating `mamba` or `conda`, before updating `--all`?\n",
      "\n",
      "Besides that there is also the `-u` flag of the installer:\n",
      "```\n",
      "-u           update an existing installation\n",
      "```\n",
      "\n",
      "3. What's the use of that and what are the differences in outcome of updating using the installer?\n",
      "\n",
      "I always do a fresh reinstall after uninstalling once in a while, but that's always a little time consuming since I also have to do all the config stuff. This is of course doable, but it would be nice, if there was one official way of keeping conda up to date.\n",
      "\n",
      "Also for this I have some questions:\n",
      "\n",
      "4. What would be the difference in outcome of a fresh reinstall vs. the `-u` way vs. the `mamba update --all` way?\n",
      "5. And what is the preferred way?\n",
      "\n",
      "I also feel it would be great, if the one official way would be mentioned in the docs.\n",
      "\n",
      "Thanks for elaborating :).\n",
      "\n",
      "Negative: 0.019\n",
      "Neutral: 0.806\n",
      "Positive: 0.176\n",
      "Compound: 0.9704\n",
      "Body: Hi everyone, I've been working as a Data Scientist for 3+ years now, mostly in telecom. I'm quite good at this, I think + I graduated from Uni with a degree in Mathematics.\n",
      "\n",
      "But I feel like I want my job (which I like) to be connected with my hobby (sports, football to be specific). On such position I would be x2 happy to work, I think. But I have no experience in sports analytics / data science (pet projects only). However, my desire to work in this field is huge.\n",
      "\n",
      "Where can I find such jobs and apply? What are my chances?   \n",
      "I am from an Eastern European country outside the EU (I think this is important).\n",
      "\n",
      "P.S.: I added a tag \"Career | Europe\", but I consider jobs worldwide.\n",
      "\n",
      "Negative: 0.053\n",
      "Neutral: 0.825\n",
      "Positive: 0.122\n",
      "Compound: 0.7674\n",
      "Body: As the title says. I found it in my functions library and have no idea if itâ€™s accurate or not (bachelors covered BStats I & II, but that was years ago); this was done from self learning. From what I understand, the 95% CI can be interpreted as guessing the mean value, while the prediction interval can be interpreted in the context of any future datapoint.\n",
      "\n",
      "Thanks and please, show no mercy.\n",
      "\n",
      "Negative: 0.019\n",
      "Neutral: 0.916\n",
      "Positive: 0.065\n",
      "Compound: 0.6597\n",
      "Body: As the title suggests, I'm curious about how Microsoft Copilot analyzes PDF files. This question arose because Copilot worked surprisingly well for a problem involving large PDF documents, specifically finding information in a particular section that could be located anywhere in the document.\n",
      "\n",
      "Given that Copilot doesn't have a public API, I'm considering using an open-source model like Llama for a similar task. My current approach would be to:\n",
      "\n",
      "1. Convert the PDF to Markdown format\n",
      "2. Process the content in sections or chunks\n",
      "3. Alternatively, use a RAG (Retrieval-Augmented Generation) approach:\n",
      "   * Separate the content into chunks\n",
      "   * Vectorize these chunks\n",
      "   * Use similarity matching with the prompt to pass relevant context to the LLM\n",
      "\n",
      "However, I'm also wondering if Copilot simply has an extremely large context window, making these approaches unnecessary.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.956\n",
      "Positive: 0.044\n",
      "Compound: 0.4329\n",
      "Body: If you are not doing regression or ML, so basically for EDA, do you transform high skewed data? If so how do you interpret it later ? As for eda working with mean/median etc. for high level insight?\n",
      "\n",
      "If not doing ML or regression is it even worth transforming to log other box cox, square root? Or we can just winsorise the data? \n",
      "\n",
      "Negative: 0.052\n",
      "Neutral: 0.876\n",
      "Positive: 0.072\n",
      "Compound: 0.1901\n",
      "Body: Stats, amazing. Math, amazing. Comp sci, amazing. But companies want problem solvers, meaning you canâ€™t get jobs based off of what you learn in college. Regardless of your degree, gpa, or â€œprojectsâ€. \n",
      "\n",
      "You need to speak â€œbusinessâ€ when selling yourself. Talk about problems you can solve, not tech or theory.\n",
      "\n",
      "Think of it as a foundation. Knowing the tech and fundamentals sets you up to â€œsolve problemsâ€ but the person interviewing you (or the higher up making the final call) typically only cares about the output. Frame yourself in a business context, not an academic one. \n",
      "\n",
      "The reason I bring up certs from the big companies is that they typically teach implementation not theory. \n",
      "\n",
      "That and were on the trail end of most â€œmigrationsâ€ where companies moved to the cloud a few years ago. They still have a few legacy on-prem solutions which they need people to shift over. Being knowledgeable in cloud platforms is indispensable in this era where companies hate on-prem. \n",
      "\n",
      "IMO most people in tech need to learn the cloud. But if youâ€™re a data scientist who knows both the modeling and implementation in a cloud company (which most companies use), youâ€™re a step above the next dude who also had a masters in comp sci and undergrad in math/stats or vice versa \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 1.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: VisionTS is new pretrained model, which transforms image reconstruction into a forecasting task.\n",
      "\n",
      "You can find an analysis of the modelÂ [here](https://aihorizonforecast.substack.com/p/visionts-building-high-performance).\n",
      "\n",
      "https://preview.redd.it/vb0vn6jit7rd1.png?width=881&format=png&auto=webp&s=9850823796c235ea2c03af79d41f296f545d1151\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Negative: 0.107\n",
      "Neutral: 0.802\n",
      "Positive: 0.092\n",
      "Compound: -0.3392\n",
      "Body: \n",
      "I am a self-taught analyst with no coding background. I do know a little bit of Python and SQL but that's about it and I am in the process of improving my programming skills. I am hired because of my background as a researcher and analyst at a pharmaceutical company. I am officially one month into this role as the sole data scientist at an ecommerce company and I am riddled with anxiety. My manager just asked me to give him a proposal for a problem and I have no clue on the solution for it. One of my colleagues who is the subject matter expert has a background in coding and is extremely qualified to be solving this problem instead of me, in which he mentioned to me that he could've handled this project. This gives me serious anxiety as I am afraid that whatever I am proposing will not be good enough as I do not have enough expertise on the matter and my programming skills are subpar. I don't know what to do, my confidence is tanking and I am afraid I'll get put on a PIP and eventually lose my job. Any advice is appreciated.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.037\n",
      "Neutral: 0.904\n",
      "Positive: 0.06\n",
      "Compound: 0.8591\n",
      "Body: Speculative sampling is the idea of using multiple models to generate output faster, less expensively than with a single large model, and with literally equivalent output as if you were using only a large model.\n",
      "\n",
      "The idea leverages a quirk of LLMs that's derived from the way they're trained. Most folks know LLMs output text autoregressively, meaning LLMs predict the next word iteratively until they've generated an entire sequence. recurrent strategies like LSTMs also used to output text autoregressively, but they were incredibly slow to train because the model needed to be exposed to a sequence numerous times to learn from that sequence.  \n",
      "  \n",
      "Transformer style LLMs use masked multi-headed self-attention to speed up training significantly by allowing the model to predict every word in a sequence as if future words did not exist. During training an LLM predicts the first, second, third, fourth, and all other tokens in the output sequence as if it were, currently, \"the next token\".\n",
      "\n",
      "Because they're trained doing this \"predict every word as the next word\" thing, they also do it during inference. There are tricks people do to modify this process to gain on efficiency, but generally speaking when an LLM generates a token at inference it also generates all tokens as if future tokens did not exist, we just usually only care about the last one.\n",
      "\n",
      "With speculative sampling/decoding (simultaneously proposed in two different papers, hence two names), you use a small LLM called the \"draft model\" to generate a sequence of a few tokens, then you pass that sequence to a large LLM called the \"target model\". The target model will predict the next token in the sequence but also, because it will predict every next tokens as if future tokens didn't exist, it will also either agree or disagree with the draft model throughout the sequence. You can simply find the first spot where the target model disagrees with the draft model, and keep what the target model predicted.\n",
      "\n",
      "By doing this you can sometimes generate seven or more tokens for every run of the target model. Because the draft model is significantly less expensive and significantly faster, this can allow for significant cost and time savings. Of course, the target model could always disagree with the draft model. If that's the case, the output will be identical to if only the target model was being run. The only difference would be a small cost and time penalty.\n",
      "\n",
      "I'm curious if you've heard of this approach, what you think about it, and where you think it exists in utility relative to other approaches.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.056\n",
      "Neutral: 0.793\n",
      "Positive: 0.151\n",
      "Compound: 0.9921\n",
      "Body: Hey everyone,\n",
      "\n",
      "I'm looking for some project suggestions, but I want to avoid the typical ones like credit card fraud detection or Titanic datasets. I feel like those are super common on every DS resume, and I want to stand out a bit more.\n",
      "\n",
      "I am a B. Applied CS student (Stats Minor) and I'm especially interested in Data Engineering (DE), Data Science (DS), or Machine Learning (ML) projects, As I am targeting DS/DA roles for my co-op. Unfortunately, I havenâ€™t found many interesting projects so far. They mention all the same projects, like customer churn, stock prediction etc.  \n",
      "  \n",
      "Iâ€™d love to explore projects that showcase tools and technologies beyond the usual suspects Iâ€™ve already worked with (numpy, pandas, pytorch, SQL, python, tensorflow, Foleum, Seaborn, Sci-kit learn, matplotlib).\n",
      "\n",
      "Iâ€™m particularly interested in working with tools like PySpark, Apache Cassandra, Snowflake, Databricks, and anything else along those lines.\n",
      "\n",
      "Edited:\n",
      "\n",
      "So after reading through many of your responses, I think you guys should know what I have already worked on so that you get an better idea.ðŸ‘‡ðŸ»\n",
      "\n",
      "This are my 3 projects:\n",
      "\n",
      "1. Predicting SpaceXâ€™s Falcon 9 Stage Landings | Python, Pandas, Matplotlib, TensorFlow, Folium, Seaborn, Power BI\n",
      "\n",
      "â€¢ Developed an ML model to evaluate the success rate of SpaceXâ€™s Falcon 9 first-stage landings, assessing its viability\n",
      "for long-duration missions, including Crew-9â€™s ISS return in February 2025.\n",
      "â€¢ Extracted and processed data using RESTful API and BeautifulSoup, employing Pandas and Matplotlib for\n",
      "cleaning, normalization, and exploratory data analysis (EDA).\n",
      "â€¢ Achieved 88.92% accuracy with Decision Tree and utilized Folium and Seaborn for geospatial analysis; created visualizations with Plotly Dash and showcased results via Power BI.\n",
      "\n",
      "2. Predictive Analytics for Breast Cancer Diagnosis | Python, SVM, PCA, Scikit-Learn, NumPy, Pandas\n",
      "â€¢ Developed a predictive analytics model aimed at improving early breast cancer detection, enabling timely diagnosis\n",
      "and potentially life-saving interventions.\n",
      "â€¢ Applied PCA for dimensionality reduction on a dataset with 48,842 instances and 14 features, improving\n",
      "computational efficiency by 30%; Achieved an accuracy of 92% and an AUC-ROC score of 0.96 using a SVM.\n",
      "â€¢ Final model performance: 0.944 training accuracy, 0.947 test accuracy, 95% precision, and 89% recall.\n",
      "\n",
      "3. (In progress) Developed XGBoost model on ~50000 samples of diamonds hosted on snowflake. Used snowpark for feature engineering and machine learning and hypertuned parameters with an accuracy to 93.46%. Deployed the model as UDF.\n",
      "\n",
      "\n",
      "Negative: 0.021\n",
      "Neutral: 0.852\n",
      "Positive: 0.127\n",
      "Compound: 0.9113\n",
      "Body: Is it only me or does anybody else find analyzing data with Excel much faster than with python or R?\n",
      "\n",
      "I imported some data in Excel and click click I had a Pivot table where I could perfectly analyze data and get an overview. Then just click click I have a chart and can easily modify the aesthetics. \n",
      "\n",
      "Compared to python or R where I have to write code and look up comments - it is way more faster for me! \n",
      "\n",
      "  \n",
      "In a business where time is money and everything is urgent I do not see the benefit of using R or Python for charts or analyses?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.933\n",
      "Positive: 0.067\n",
      "Compound: 0.25\n",
      "Body: Meta released Llama3.2 a few hours ago providing Vision (90B, 11B) and small sized text only LLMs (1B, 3B) in the series. Checkout all its details here : https://youtu.be/8ztPaQfk-z4?si=KoCOpWQ5xHC2qtCy\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 1.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: What are you moving from/to?\n",
      "\n",
      "E.g., we recently went from MS SQL Server to Redshift. 500+ person company.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: \n",
      "\n",
      "Negative: 0.043\n",
      "Neutral: 0.957\n",
      "Positive: 0.0\n",
      "Compound: -0.128\n",
      "Body: Is it still relevant to be learning and using huggingface models and the ecosystem vs pivoting to a langchain llm api? Feel the majomajor AI modeling companies are going to dominate the space soon. \n",
      "\n",
      "Negative: 0.099\n",
      "Neutral: 0.847\n",
      "Positive: 0.054\n",
      "Compound: -0.4767\n",
      "Body: I might post this elsewhere as well, cause Iâ€™m in a conference where theyâ€™re discussing AI â€œstandardsâ€, IEEE 7000, CertifAIed, ethics, blah blah blahâ€¦\n",
      "\n",
      "But I have no personal experience with anyone in any tech company following NIST standards for anything. I also do not see any consequences for NOT following these standards. \n",
      "\n",
      "Has anyone become certified in these standards and had a real net-benefit outcome for their business or their career?\n",
      "\n",
      "This feels like a massive waste of time and effort. \n",
      "\n",
      "Negative: 0.024\n",
      "Neutral: 0.877\n",
      "Positive: 0.099\n",
      "Compound: 0.7757\n",
      "Body: Has anyone ever used or seen used the principles of Applied Information Economics created by Doug Hubbard and described in his book How to Measure Anything?\n",
      "\n",
      "They seem like a useful set of tools for estimating things like timelines and ROI, which are often notoriously difficult for exploratory data science projects. However, I canâ€™t seem to find much evidence of them being adopted. Is this because there is a flaw Iâ€™m not noticing, because the principles have been co-opted into other frameworks, just me not having worked at the right places, or for some other reason?\n",
      "\n",
      "Negative: 0.016\n",
      "Neutral: 0.921\n",
      "Positive: 0.062\n",
      "Compound: 0.7096\n",
      "Body: Hello everyone, Iâ€™ve been working in IT in non-technical roles for over a decade, though I donâ€™t have a STEM-related educational background. Recently, Iâ€™ve been looking for ways to advance my career and came across a Data Science MS program at Eastern University that can be completed in 10 months for under $10k. While I know there are more prestigious programs out there, Iâ€™m not in a position to invest more time or money. Given my situation, would it be worth pursuing this program, or would it be better to drop the idea? I searched for this topic on reddit, and found that most of the comments mention pretty much the same thing as if they are being read from a script. \n",
      "\n",
      "Negative: 0.027\n",
      "Neutral: 0.9\n",
      "Positive: 0.074\n",
      "Compound: 0.8423\n",
      "Body: I'm quite interested by the current trends about no code / low code GenAI :\n",
      "\n",
      "* Models are becoming more versatile and multimodal = They can ingest almost any type of content / data\n",
      "* Auto-embedding and Auto-RAG features are becoming better and more accessible (GPT Builder, \"Projects\" from Anthropic...), reducing the need for AI engineering, and with less and less limitations on the type and quantity of content that can be added\n",
      "* Fine-tuning can be done directly by myself, the meta-prompts is added to the \"AI assistant\" with standard features\n",
      "\n",
      "At the same time, I feel a lot of companies are still organizing their \"GenAI Engineering\" capabilities , still upskilling, trying not to get outrun by the fast pace of innovation & the obsolescence of some products or approaches, and with the growing demand from the users, the bottleneck is getting bigger.\n",
      "\n",
      "So, my feeling is we'll see more and more use cases fully covered by standard features and less and less work for AI Architect and AI Engineers, with the exception of complex ecosystem integration,, agentic on complex processes, specific requirements like real time, high number of people etc.\n",
      "\n",
      "What do you think? What's the future of AI Architecture & Engineering?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.982\n",
      "Positive: 0.018\n",
      "Compound: 0.0772\n",
      "Body: I am working as a data scientist for a year now. I want to transition to MLE or SDE in AI/ML kind of roles going down the lane. Is it possible for me to do so and what all are expected for these kind of roles? \n",
      "\n",
      "Currently I am working on building forecasting models and some Generative AI. I don't have exposure to model deployment or ML system building as of now. \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.877\n",
      "Positive: 0.123\n",
      "Compound: 0.962\n",
      "Body: Hello my fellow DS peeps,\n",
      "\n",
      "I'm building a model where my historical data that will be used in training is in a different resolution between actuals and forecasts. For example, I have hourly forecasted Light Rainfall, Moderate Rainfall, and Heavy Rainfall. During this same time period, I have actuals only in total rainfall amount.\n",
      "\n",
      "Couple of questions:\n",
      "\n",
      "- Has anyone ever used historical forecast data rather than actuals as training data and built a successful model out on that? We would be removed one layer from truth, but my actuals are in a different resolution. I can't say much about my analysis,but there is merit in taking into account the kind of rainfall.\n",
      "\n",
      "- Would it just be better if I trained model on actuals and then feed in as inputs the sum of my forecasted values (Light/Med/Heavy)? \n",
      "\n",
      "Looking to any recommendations you may have. Thanks!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.715\n",
      "Positive: 0.285\n",
      "Compound: 0.9605\n",
      "Body: [dash-react-simple-maps](https://i.redd.it/zfn2gg1svsqd1.gif)\n",
      "\n",
      "Hi, r/datascience!\n",
      "\n",
      "I want to present my new library for creating maps with Dash: [dash-react-simple-maps](https://github.com/ploomber/dash-react-simple-maps).\n",
      "\n",
      "As the name suggests, it uses the fantastic [react-simple-maps](https://www.react-simple-maps.io/) library, which allows you to easily create maps and add colors, annotations, markers, etc.\n",
      "\n",
      "Please take it for a spin and share your feedback. This is my first Dash component, so Iâ€™m pretty stoked to share it!\n",
      "\n",
      "Live demo: [dash-react-simple-maps.ploomberapp.io](https://dash-react-simple-maps.ploomberapp.io/)\n",
      "\n",
      "Negative: 0.036\n",
      "Neutral: 0.798\n",
      "Positive: 0.166\n",
      "Compound: 0.9808\n",
      "Body: Hey folks,\n",
      "\n",
      "dlt cofounder here. dlt is a python library for loading data, and we are offering some OSS but also commercial functionality for achieving compliance.  \n",
      "  \n",
      "We heard from a large chunk of our community thatÂ **you hate governance**Â but want to learn how to do it right. Well, it's no data science, so we arranged to have a professional lawyer/data protection officer give a webinar for data professionals, to help them achieve compliance.   \n",
      "  \n",
      "Specifically, we will do one run for GDPR and one for HIPAA. There will be space for Q&A and if you need further consulting from the lawyer, she comes highly recommended by other data teams. We will also send you afterwards a compliance checklist and a cheatsheet-notebook-demo you can self explore of the dlt OSS functionality for helping with GDPR.  \n",
      "  \n",
      "If you are interested, sign up here:Â [https://dlthub.com/events](https://dlthub.com/events).\n",
      "\n",
      "Of course, this learning content is free :) You will see 2 slides about our commercial offering at the end (just being straightforward).  \n",
      "  \n",
      "Do you have other learning interests around data ingestion?\n",
      "\n",
      "Please let me know and I will do my best to make them happen.\n",
      "\n",
      "Negative: 0.038\n",
      "Neutral: 0.864\n",
      "Positive: 0.098\n",
      "Compound: 0.9394\n",
      "Body: I have a set (\\~250) of broken units and I want to understand why they broke down. Technical experts in my company have come up with hypotheses of why, e.g. \"the units were subjected to too high or too low temperatures\", \"units were subjected to too high currents\" etc. I have extracted a set of features capturing these events in a time period before the the units broke down, e.g. \"number of times the temperature was too high in the preceding N days\" etc. I also have these features for a control group, in which the units did not break down.\n",
      "\n",
      "My plan is to create a set of (ML) models that predicts the target variable \"broke\\_down\" from the features, and then study the variable importance (VIP) of the underlying features of the model with the best predictive capabilities. I will not use the model(s) for predicting if so far working units will break down. I will only use my model for getting closer to the root cause and then tell the technical guys to fix the design.\n",
      "\n",
      "For selecting the best method, my plan is to split the data into test and training set and select the model with the best performance (e.g. AUC) on the test set. \n",
      "\n",
      "My question though is, should I analyze the VIP for this model, or should I retrain a model on all the data and use the VIP of this? \n",
      "\n",
      "As my data is quite small (\\~250 broken, 500 control), I want to use as much data as possible, but I do not want to risk overfitting either. What do you think?\n",
      "\n",
      "Thanks\n",
      "\n",
      "Negative: 0.126\n",
      "Neutral: 0.817\n",
      "Positive: 0.057\n",
      "Compound: -0.3098\n",
      "Body: Lots of headcount, worth applying with a referral. 3 days RTO policy.\n",
      "\n",
      "Edit: I don't work there please stop asking me for referrals. Just heard this news through the grapevines. \n",
      "\n",
      "Negative: 0.111\n",
      "Neutral: 0.862\n",
      "Positive: 0.027\n",
      "Compound: -0.4791\n",
      "Body: I want to change from a data science role to machine learning engineering.\n",
      "\n",
      "I think data science jobs are mostly disorganized. And it's always hard to know how the job will be. \n",
      "\n",
      "My job as DS here is most to monitor our model. Not create experiments. \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.891\n",
      "Positive: 0.109\n",
      "Compound: 0.1511\n",
      "Body: Im 30 minutes into this call and I want to claw my eyes out--help!\n",
      "\n",
      "Negative: 0.017\n",
      "Neutral: 0.948\n",
      "Positive: 0.035\n",
      "Compound: 0.3182\n",
      "Body: I'm building a financial forecast and for the life of me cannot figure out how to get started. Here's the data model:\n",
      "\n",
      "|table\\_1|description|\n",
      "|:-|:-|\n",
      "|account\\_id||\n",
      "|year|calendar year|\n",
      "|revenue|total spend|\n",
      "\n",
      "\n",
      "\n",
      "|table\\_2|description|\n",
      "|:-|:-|\n",
      "|account\\_id||\n",
      "|subscription\\_id||\n",
      "|product\\_id||\n",
      "|created\\_date|date created|\n",
      "|closed\\_date||\n",
      "|launch\\_date|start of forecast\\_12\\_months|\n",
      "|subsciption\\_type|commitment or by usage|\n",
      "|active\\_binary||\n",
      "|forecast\\_12\\_months|expected 12 month spend from launch date|\n",
      "|last\\_12\\_months\\_spend|amount spent up to closed\\_date|\n",
      "\n",
      "The ask is to build a predictive model for revenue. I have no clue how to get started because the forecast\\_12\\_months and last\\_12\\_months\\_spend start on different dates for all the subscription\\_ids across the span of like 3 years. It's not a full lookback period (ie, 2020-2023 as of 9/23/2024).\n",
      "\n",
      "Any idea on how you'd start this out? The grain and horizon are up to you to choose.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.746\n",
      "Positive: 0.254\n",
      "Compound: 0.9546\n",
      "Body: I am currently a junior DS in the GenAI team of a well known company. I have been approached for an interview for the Senior Gen AI Solutions Architect at Amazon. Is this possible worth the switch? Pros look like this is a senior position. Cons looks like my field gets switched from data science (which I really like) to solutions architecture. Should I go ahead with this job if I clear the interviews? (Please advise). \n",
      "\n",
      "Negative: 0.016\n",
      "Neutral: 0.819\n",
      "Positive: 0.164\n",
      "Compound: 0.9482\n",
      "Body: I have been using Gemini, meta and Claude for various purposes and honestly Claude has been the best amongst these. \n",
      "\n",
      "**Pros**  \n",
      "I get to learn new functions, new styles of coding, new concepts etc. Also helps me to construct and proof read my resumes and applications better. And then some.  \n",
      "\n",
      "\n",
      "**Cons**:\n",
      "\n",
      "Limited Message count per day\n",
      "\n",
      "  \n",
      "At this point, I was considering getting a premium subscription. although it is a bit expensive when converted to my local currency. \n",
      "\n",
      "I was wondering if anyone has better suggestions for AI tools, not just limited to coding. Or share their experience with premium subscriptions of such AI models.\n",
      "\n",
      "Negative: 0.107\n",
      "Neutral: 0.743\n",
      "Positive: 0.15\n",
      "Compound: 0.7717\n",
      "Body: I'm training a neural network for a computer vision project, i started with simple layers i noticed that it is not enough, i added some convolutional layers i ended up facing overfitting, training accuracy and loss was beyond great than validation's i tried to augment my data, overfitting was gone but the model was just bad ... random guessing bad, i then decided to try transfer learning, training accuracy and validation were just Great, but the training loss was waaaaay smaller than the validation's like 0.0001 for training and 1.5 for validation a clear sign of overfitting. I tried to adjust the learning rate, change the architecture change the optimizer but i guess none of that worked. I'm new and i honestly have no idea how to tackle this.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.816\n",
      "Positive: 0.184\n",
      "Compound: 0.6597\n",
      "Body: Mistral AI has started rolling out free LLM API for developers. Check this demo on how to create and use it in your codes : https://youtu.be/PMVXDzXd-2c?si=stxLW3PHpjoxojC6\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.944\n",
      "Positive: 0.056\n",
      "Compound: 0.6696\n",
      "Body:  \n",
      "\n",
      "Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n",
      "\n",
      "* Learning resources (e.g. books, tutorials, videos)\n",
      "* Traditional education (e.g. schools, degrees, electives)\n",
      "* Alternative education (e.g. online courses, bootcamps)\n",
      "* Job search questions (e.g. resumes, applying, career prospects)\n",
      "* Elementary questions (e.g. where to start, what next)\n",
      "\n",
      "While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.874\n",
      "Positive: 0.126\n",
      "Compound: 0.9713\n",
      "Body: When I graduated from University, I took a job as a customer service representative, because I needed the money.\n",
      "\n",
      "I had a degree in Computer Science with a specialization in ML, so I was obviously overqualified, but I couldnâ€™t afford to wait around. After automating some of their tasks and identifying other areas in which I could generate business value, I convinced the CEO to hire me as a Data Analyst. This is how I eventually became a Data Scientist (Iâ€™ve been working in Data & analytics for the past 7 years now).\n",
      "\n",
      "**Has anyone else also managed to successfully turn their non-data-related job (perhaps non-technical) into a data role, like data analyst or data scientist, within the same company?**\n",
      "\n",
      "How did you make the switch, and what were the challenges or strategies that helped you along the way?\n",
      "\n",
      "Iâ€™d love to hear your story, Iâ€™m doing some research for an article Iâ€™m writing for my [newsletter](https://tobeadatascientist.substack.com/)\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.825\n",
      "Positive: 0.175\n",
      "Compound: 0.9259\n",
      "Body: PerpetualBooster v0.4.7: Multi-threading & Quantile Regression\n",
      "\n",
      "Excited to announce the release of PerpetualBooster v0.4.7! \n",
      "\n",
      "This update brings significant performance improvements with multi-threading support and adds functionality for quantile regression tasks. PerpetualBooster is a hyperparameter-tuning-free GBM algorithm that simplifies model building. Similar to AutoML, control model complexity with a single \"budget\" parameter for improved performance on unseen data.\n",
      "\n",
      "\n",
      "Easy to Use:\n",
      "```python\n",
      "from perpetual import PerpetualBooster\n",
      "model = PerpetualBooster(objective=\"SquaredLoss\")\n",
      "model.fit(X, y, budget=1.0)\n",
      "```\n",
      "\n",
      "Install:\n",
      "pip install perpetual\n",
      "\n",
      "\n",
      "Github repo:\n",
      "[https://github.com/perpetual-ml/perpetual](https://github.com/perpetual-ml/perpetual)\n",
      "\n",
      "Negative: 0.057\n",
      "Neutral: 0.787\n",
      "Positive: 0.156\n",
      "Compound: 0.9592\n",
      "Body: I'm trying to predict if a user is going to convert or not. I've used Xgboost model, augmented data for minority class using samples from previous dates so model can learn. The ratio right now is at 1:700. I also used scale_pos_weight to make model learn better. Now, the model achieves 90% recall for majority class and 80% recall for minority class on validation set. Precision for minority class is 1% because 10% false positives overwhelm it. False positives have high engagement rate just like true positives but they don't convert easily that's what I've found using EDA (FPs can be nurtured given they built habit with us so I don't see it as too bad of a thing ) \n",
      "\n",
      "1. My philosophy is that model although not perfect has reduced the search space to 10% of total users so we're saving resources. \n",
      "2. FPs can be nurtured as they have good engagement with us. \n",
      "\n",
      "Do you think I should try any other approach? If so suggest me one or else tell me how do I convince manager that this is what I can get from model given the data. Thank you!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.902\n",
      "Positive: 0.098\n",
      "Compound: 0.3612\n",
      "Body: Many of the things we do are quite complex. At the end of the day or week I feel like my brain has melted.\n",
      "\n",
      "Negative: 0.045\n",
      "Neutral: 0.764\n",
      "Positive: 0.191\n",
      "Compound: 0.5825\n",
      "Body: Is it very good news? Somewhat good news? No effect at all? \n",
      "\n",
      "I would guess that this is somewhat good news, but I don't expect any drastic changes overnight.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.798\n",
      "Positive: 0.202\n",
      "Compound: 0.9168\n",
      "Body: Which is more important to you? I am needing to decide between staying at my current job, where I have been told I'll be getting a promotion early next year and hopefully a moderate salary increase, or a new job at a 20% salary increase, where I wouldn't be getting the same promotion for 4-6 years.\n",
      "\n",
      "\n",
      "Just curious what people seem to care about more.\n",
      "\n",
      "Negative: 0.012\n",
      "Neutral: 0.938\n",
      "Positive: 0.05\n",
      "Compound: 0.7745\n",
      "Body: Newbie to data analysis here. I have been learning python and various data wrangling techniques for the last 4 or 5 years. I am finally getting around to clustering, and am having trouble deciding which to use as my go to method between the various types. The methods I have researched so far:\n",
      "- k means\n",
      "- dbscan\n",
      "- optics\n",
      "- pca with svd\n",
      "- ica\n",
      "\n",
      "I like understanding something fully before implementing it, and the concept of hierarchical clustering is intriguing to me. But the math behind it, and with clustering methods in general (eg, distancing method for optics) I just canâ€™t wrap my head around.\n",
      "\n",
      "Are there any resources / short classes / YouTube videos etc that can break this down in simple terms, or is really all research papers that can explain what these techniques do and when to use em?\n",
      "\n",
      "TIA!\n",
      "\n",
      "Negative: 0.138\n",
      "Neutral: 0.796\n",
      "Positive: 0.066\n",
      "Compound: -0.959\n",
      "Body: Have an interview with a candidate, i am absolutely positive the person is lying and is straight up making up the role that they have.\n",
      "\n",
      "Their achievements are perfect and identical to the job posting but their linkedin job title is completely unrelated to the role and responsibilities that they have on the application. We are talking marketing analytics vs risk modeling. \n",
      "\n",
      "Is it normal to cancel the interview before it even happens?\n",
      "\n",
      "Also i worked with the employer and the person claims projects but these projects literally span 2 different departments and I actually know the people in there.\n",
      "\n",
      "\n",
      "Edit: further clarify, the person is claiming the achievements of  3-4 departments. Very high level but clearly has nothing to show with actual skills specific to the job. My problem is the person lying on the \n",
      "application.\n",
      "\n",
      "My problem is them not being ethical.\n",
      "\n",
      "Edit 2: it gets even worse, person claims they are a leading expert and actually teaches the specific job that we do in university. I looked him up in the university, the person does not teach any courses related at all.  I am 100% sure they are lying no way another easily verifiable thing is a lie. Especially when its 5+ years. \n",
      "\n",
      "\n",
      "Negative: 0.027\n",
      "Neutral: 0.815\n",
      "Positive: 0.158\n",
      "Compound: 0.9726\n",
      "Body: I have a binary classification model that I have trained with balanced classes, 5k positives and 5k negatives. When I train and test on 5 fold cross validated data I get F1 of 92%. Great, right? The problem is that in the real world data the positive class is only present about 1.7% of the time so if I run the model on real world data it flags 17% of data points as positive. My question is, if I train on such a tiny amount of positive data it's not going to find any signal, so how do I get the model to represent the real world quantities correctly? Can I put in some kind of a weight? Then what is the metric I'm optimizing for? It's definitely not F1 on the balanced training data. I'm just not sure how to get at these data proportions in the code.\n",
      "\n",
      "\n",
      "Negative: 0.082\n",
      "Neutral: 0.906\n",
      "Positive: 0.012\n",
      "Compound: -0.9456\n",
      "Body: I'm an EU citizen and I have 3 years of experience as a data scientist and I have a master's in mathematics.\n",
      "\n",
      "I have been applying for jobs for quite a while now. Rarely do I apply to jobs in Eastern Europe (where I live), but when I do, I usually get an HR interview. I also get a lot of unsolicited linkedin messages from recruiters in my area as well. So I think my CV/LinkedIn profile is at least halfway decent, although I rewrote my CV three times besides constantly updating .\n",
      "\n",
      "However, I have probably applied to hundreds of jobs in Western European countries with little to no luck, especially the past 12 months or so. This week I asked somebody I know through an open source repo to refer me to his multinational company in Berlin. Today I got an automated rejection email, so I'm getting hopeless.\n",
      "\n",
      "How do you even get a job abroad? Do I just have to wait to get more experience? Should I apply for a PhD and make less than what I make now for the next 3 years? Also, is it less hopeless to get a job in the UK or in the US?\n",
      "\n",
      "Negative: 0.038\n",
      "Neutral: 0.873\n",
      "Positive: 0.089\n",
      "Compound: 0.7219\n",
      "Body: Recently: A medium-sized manufacturing company hired a data scientist to use data from production and its systems. The aim is to derive improvement projects and initiatives. Some optimization initiatives have been launched. \n",
      "\n",
      "\n",
      "\n",
      "Then: The company has been struggling with falling sales for six months, so it decided to take a closer look at the personnel roster to reduce costs. They asked themselves â€œDo we really need this employee?â€ for each position.\n",
      "\n",
      "When arrived at the data scientist position, they decided to give up this position. \n",
      "\n",
      "Do you understand the decision? Do you think that a data scientist is just a nice to have when things are running smoothly? \n",
      "\n",
      "\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.778\n",
      "Positive: 0.222\n",
      "Compound: 0.8402\n",
      "Body: I am considering MLOps but I need expert opinion on what skills are necessary and if there are any reliable courses that can help me? \n",
      "\n",
      "Any advice would be appreciated. \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.862\n",
      "Positive: 0.138\n",
      "Compound: 0.5994\n",
      "Body: Does somebody know some resources where I can see/read about data science projects successfully implemented in practice? \n",
      "\n",
      "I feel that 90% of people just talk about gaining insights and improving decisions, but I rarely read about such projects in practice.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 1.0\n",
      "Positive: 0.0\n",
      "Compound: 0.0\n",
      "Body: In my second year as an analyst im realizing that having the right numbers 100% and not 85% of the time makes a big difference i. credibility.\n",
      "\n",
      "Negative: 0.027\n",
      "Neutral: 0.926\n",
      "Positive: 0.047\n",
      "Compound: 0.5376\n",
      "Body: Is it \"Lie-Curt\", or \"Lick-Urt\"?\n",
      "\n",
      "I've mostly heard the former, but an old psych prof told me it's the latter.\n",
      "\n",
      "(If you don't know, \"Likert Scores\" is the formal word for integer ranking scores from respondents, such as \"rate this movie from 1 to 5 stars, with '5' being the best.)\n",
      "\n",
      "EDIT:  I wanted to post this in response to several different threads:  this scale is named after Rensis Likert, a psychologist who developed it in 1932.   My point is that, since it's a last name, there IS a right answer to the question, it's not a popularity contest.  It should be the way this guy pronounced his name.  I just don't know what that is.\n",
      "\n",
      "Negative: 0.022\n",
      "Neutral: 0.887\n",
      "Positive: 0.09\n",
      "Compound: 0.9109\n",
      "Body: As a student learning data analysis, Iâ€™m curiousâ€”once a data analyst automates the ETL processes and sets up dashboards, what do they actually do on a daily basis? It seems like you wouldnâ€™t be doing full data analysis and reporting every day. Do most of the tasks involve monitoring pipelines, updating dashboards, or handling ad hoc requests? Iâ€™d love to understand more about what the day-to-day work looks like!\n",
      "\n",
      "Also, Iâ€™ve been thinkingâ€”once all the data processes are automated and the company has access to dashboards and reports, what stops them from not needing the analyst anymore? Iâ€™m concerned that after setting everything up, I could be seen as unnecessary, since the tools and systems would keep running on their own. How do data analysts continue to add value and avoid being let go once automation is in place? Itâ€™s something thatâ€™s been on my mind as I try to figure out what the long-term role looks like.\n",
      "\n",
      "Negative: 0.026\n",
      "Neutral: 0.82\n",
      "Positive: 0.154\n",
      "Compound: 0.9975\n",
      "Body: I recently had a conversation with the founder of Arize, an AI testing and observability platform. he said something interesting which I'll paraphrase:\n",
      "\n",
      "\"ReAct agents aren't successful in production because they're too vague. More constrained agents, like graph based agents, have been doing much better\". - [source](https://www.youtube.com/watch?v=N59Z7uJ8DDA&list=PLPaLTf0W0L_P8p5GmJFBSwEIr0PinWPpH&index=2)\n",
      "\n",
      "talking about agents with a company focused in AI observability and testing was a breath of fresh air, and had me thinking of agents in a new and radical way: Like they're software which needs to be testable.\n",
      "\n",
      "For those of you who don't know, LangGraph is a new set of tooling by LangChain which allows you to structure an agent as a directed graph. There are nodes which allow you to do operations, edges which allow you to chain operations together, and decision edges which allow you to make a decision based on some criteria. I think there are a few ways to actually make these graphs, but I'm only familiar with the \"state graph\", which allows you to define some state object (which is a dictionary with a bunch of default values) that gets passed around throughout the graph. This allows you to do things like:\n",
      "\n",
      "* Keep track of the conversational flow\n",
      "* Keep track of key parsed data\n",
      "* Keep track of explicit application logic\n",
      "* Work with stateless API setups, presumably, because the state is atomic and serializable\n",
      "* Employ explicit programmatic steps in the conversation/process seamlessly.\n",
      "\n",
      "Here's an example of a conversation with a graph based agent I created. Notice how well it marries strict programatic rules and strange conversational edge cases: something more loosely defined agents like ReAct struggle with.\n",
      "\n",
      "    Hi there! I'm Rachael, and I'm excited to help you get set up with a new home. Could you please provide me with your full name so we can get started?\n",
      "    dan w\n",
      "    Hi Dan, it looks like your name might be incomplete. Could you please provide your full name or confirm if \"Dan W\" is indeed your full name? This will help me get started on finding the perfect home for you.\n",
      "    yeah beleive it or not that's actually my full name. Weird parents.\n",
      "    Nice to meet you, Dan W! Do you already have an account set up with us?\n",
      "    yep\n",
      "    Great, Dan! Could you please provide the email address associated with your account?\n",
      "    yeah it's \n",
      "    Great, Dan! A short code has been sent to your email. Could you please check your inbox and provide me with that code to confirm your email?\n",
      "    just got it. 1111\n",
      "    Email verified. Done!\n",
      "    output state:\n",
      "    {'first_name': 'Dan', 'last_name': 'W', 'incrementor': 5, 'conversation':...}hire@danielwarfield.dev\n",
      "\n",
      "[source, with code](https://iaee.substack.com/p/langgraph-intuitively-and-exhaustively)\n",
      "\n",
      "The fact that this conversation is, under the hood, structured as a directed graph allows me to modify key points in the conversation explicitly, rather than contend with a single massive prompt which governs the whole conversation.\n",
      "\n",
      "Iâ€™ve had a lot of conversations with some heavy hitters in the industry over the last few months, and Iâ€™m seeing a regular theme: If AI canâ€™t transcend the twitter demo and become actual useful products, then the industry is in a world of hurt. Luckily, I think graph based agents are the right balance of abstract and specific to solve a lot of conversational use cases. I expect weâ€™ll see them grow as a fundamental component of modern LLM powered applications.\n",
      "\n",
      "Negative: 0.021\n",
      "Neutral: 0.911\n",
      "Positive: 0.069\n",
      "Compound: 0.5346\n",
      "Body: Hi there,\n",
      "\n",
      "  \n",
      "My boss wants me to create a segmentation of current and new/potential customers based on psychographic/attitudinal profiles so that we can create targeted marketing for our products. He then wants to append a segment to everyone in our CRM. \n",
      "\n",
      "  \n",
      "I'm struggling to work out how to link the two things though. We can do a survey with a survey panel, run the segmentation, but how do I then link that to the CRM data (we have the usual variables, like demographics and purchase history). Any advice?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.929\n",
      "Positive: 0.071\n",
      "Compound: 0.8834\n",
      "Body: I built a model to predict next week's TSA passenger volumes using only historical data. I am doing this to inform my trading on prediction markets. [I explain the background here for anyone interested. ](https://ferraijv.github.io/kalshi_tsa_trading_bot_overview/)\n",
      "\n",
      "The goal is to predict weekly average TSA passengers for the next week Monday - Sunday.\n",
      "\n",
      "Right now, my model is very simple and consists of the following:\n",
      "\n",
      "1. Find weekly average for the same week last year day of week adjusted\n",
      "2. Calculate prior 7 day YoY change\n",
      "3. Find most recent day YoY change \n",
      "4. My multiply last year's weekly average by the recent YoY change. Most of it weighted to 7 day YoY change with some weighting towards the most recent day\n",
      "5. To calculate confidence levels for estimates, I use historical deviations from this predicted value. \n",
      "\n",
      "How would you improve on this model either using external data or through a different modeling process?\n",
      "\n",
      "Negative: 0.039\n",
      "Neutral: 0.852\n",
      "Positive: 0.109\n",
      "Compound: 0.9775\n",
      "Body: I'm in a bit of a pickle (admittedly, a total luxury problem) and could use some community wisdom. I work as a data scientist, and I often work with large local datasets, primarily in R, and I'm facing a decision about my work machine. I  recognize this is a privilege to even consider, but I'd still really appreciate your insights.\n",
      "\n",
      "**Current Setup:**\n",
      "\n",
      "* MacBook Pro M1 Max with 64GB RAM, 10 CPU and 32 GPU cores\n",
      "* I do most of my modeling locally\n",
      "* Often deal with very large datasets\n",
      "\n",
      "**Potential Upgrade:**\n",
      "\n",
      "* Work is offering to upgrade me to a MacBook Pro M3 Max\n",
      "* It comes with 48GB RAM, 16 CPU cores, 40 GPU cores\n",
      "* We're a small company, and circumstances are such that this specific upgrade is available now. It's either this or wait an undetermined time for the next update.\n",
      "\n",
      "**Current Usage:**\n",
      "\n",
      "* Activity Monitor shows I'm using about 30-42GB out of 64GB RAM\n",
      "* R session is using about 2.4-10GB\n",
      "* Memory pressure is green (efficient use)\n",
      "* I have about 20GB free memory\n",
      "\n",
      "**My Concerns:**\n",
      "\n",
      "1. Will losing 16GB RAM impact my ability to handle large datasets?\n",
      "2. Is the performance boost of M3 worth the RAM trade-off?\n",
      "3. How future-proof is 48GB for data science work?\n",
      "\n",
      "I'm torn because the M3 is newer and faster, but I'm somewhat concerned about the RAM reduction. I'd prefer not to sacrifice the ability to work with large datasets or run multiple intensive processes. That said, I really like the idea of that shiny new M3 Max.\n",
      "\n",
      "For those of you working with big data on Macs:\n",
      "\n",
      "* How much RAM do you typically use?\n",
      "* Have you faced similar upgrade dilemmas?\n",
      "* Any experiences moving from higher to lower RAM in newer models?\n",
      "\n",
      "Any insights, experiences, or advice would be greatly appreciated. \n",
      "\n",
      "Negative: 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral: 0.934\n",
      "Positive: 0.036\n",
      "Compound: -0.2477\n",
      "Body: Right now, I have 3 job offers on the table. One from the Patent+Trademark Office, one from the FCC, and one from a private gov't consulting firm. I don't think I'll take the one from the PTO, but the FCC/Private choice has me hung up. \n",
      "\n",
      "\n",
      "The FCC job is my current shortest path to my goal: federal work as high as I can handle. I'd prefer the GAO to anything else, but I'll take what I can get. The work at the FCC isn't particularly \"data science\"-y either, but it starts my climb up the GS scale so that's a big 'plus.\n",
      "\n",
      "\n",
      "The private job salary beats the FCC one by a lot (~$30k), and it's for sure nuts-and-bolts data science doing text modelling (something that's also a goal of mine: staying technically on the forefront). But it doesn't really get me much closer to my main goal of federal work.\n",
      "\n",
      "\n",
      "My favorite 'job lead' I have right now is currently in scheduling hell and probably won't come out of that before I actually need to start having income again. So if that one comes through, I'll take that one above any of the current offers.\n",
      "\n",
      "\n",
      "Thoughts? Articles? Blogs?\n",
      "\n",
      "Negative: 0.138\n",
      "Neutral: 0.799\n",
      "Positive: 0.063\n",
      "Compound: -0.9436\n",
      "Body: Howdy folks,\n",
      "\n",
      "I was let go about two months ago and at times been applying and at times not as much. Im trying to get back to it and noticing that um.....where there maybe used to be 200 job postings within my parameters....there's about a NINETY percent drop in jobs available?!? Im on indeed btw.\n",
      "\n",
      "Now, maybe thats due to checking yesterday (Monday), but Im checking this today and its not really that much better AT ALL. Usually Tuesday is when more roles are posted on/by.\n",
      "\n",
      "Im aware the job market has been wonky for a while (Im not oblivious) but it was literally NOTHING close to this like a month ago. This is kind of terrifying and sobering as hell to see.\n",
      "\n",
      "Is anyone else seeing the same? This seems absolutely insane.\n",
      "\n",
      "Just trying to verify if its maybe me/something Im doing or if others are seeing the same VERY low numbers? Like where I maybe saw close to 200 positions open, Im not seeing like 25 or 10 MAX.\n",
      "\n",
      "Negative: 0.028\n",
      "Neutral: 0.93\n",
      "Positive: 0.042\n",
      "Compound: -0.1556\n",
      "Body: \n",
      "Is it possible for propensity score matching to fail to find a control for certain test subjects?  \n",
      "\n",
      "In my situation, I am trying to compare the conversion rate between 2 groups, test group has treatment but control group doesnâ€™t.  I want to get them to be balanced. \n",
      "\n",
      "But I am trying to figure out what if not every subject in the test group (with N=1000) has a match.  What can I still say about the treatment effect size?  \n",
      "\n",
      "\n",
      "Negative: 0.05\n",
      "Neutral: 0.858\n",
      "Positive: 0.092\n",
      "Compound: 0.8271\n",
      "Body: I've inherited an ML repository from a previous employee, and I've been tasked with refactoring the code to reproduce the final results they had previously, and to make it simpler and easier for our team and others to adapt to similar projects.\n",
      "\n",
      "  \n",
      "In some ways, I'm inheriting a lot of solutions: the previous person was clever and had produced a good model. However, I'm inheriting a lot of problems, too: e.g., a messy repo with roughly 50 scripts, very idiosyncratic coding practices, unaddressed TODOs, lines commented out for no explained reason, internal redundancies, lack of docstrings, a very minimal README, and no document explaining how to use the repository for the next person.\n",
      "\n",
      "  \n",
      "Luckily, my new team has been very understanding and the expectations are not unrealistic: I have been given a lot  of runway to figure things out and the team is aware the codebase is a mess. But this is the first time I've had to refactor such a large codebase like this and I'm feeling a bit overwhelmed getting it all in my head, especially with so little documentation.\n",
      "\n",
      "  \n",
      "How do you suggest approaching a situation like this? \n",
      "\n",
      "Negative: 0.027\n",
      "Neutral: 0.814\n",
      "Positive: 0.159\n",
      "Compound: 0.9559\n",
      "Body: As I become more mature in my career, Iâ€™m also learning what I like in a company/role. Iâ€™m currently a data scientist in the consulting world. I realized that I donâ€™t really like it because I donâ€™t enjoy external client work, Iâ€™m not a fan of the ebb and flow nature of the work and honestly I hate having to do extracurriculars (but maybe thatâ€™s unavoidable?). \n",
      "\n",
      "As I look to the future, I want to learn more about the other industries out there so tell me, how would you describe your industry? What kind of work do you do? What kind of work do you NOT do? Does it keep you busy? I would especially love to hear from people working in retail marketing :)\n",
      "\n",
      "Negative: 0.092\n",
      "Neutral: 0.797\n",
      "Positive: 0.111\n",
      "Compound: 0.9118\n",
      "Body: I was recently in a secret demo run by the Cuda and Polars team. They passed me through a metal detector, put a bag over my head, and drove me to a shack in the woods of rural France. They took my phone, wallet, and passport to ensure I wouldnâ€™t spill the beans before finally showing off what theyâ€™ve been working on.\n",
      "\n",
      "Or, thatâ€™s what it felt like. In reality it was a zoom meeting where they politely asked me not to say anything until a specified time, but as a tech writer the mystery had me feeling a little like James Bond.\n",
      "\n",
      "The tech they unveiled was something a lot of data scientists have been waiting for: Dataframes with GPU acceleration capable of real time interactive data exploration on 100+GBs of data. Basically, all you have to do is specify the GPU as the preferred execution engine when calling .collect() on a lazy frame, and GPU acceleration will happen automagically under the hood. I saw execution times that took around 20% the time as CPU computation in my testing, with room for even more significant speed increases in some workloads.\n",
      "\n",
      "I'm not affiliated with CUDA or Polars in any way as of now, though I do think this is very exciting.\n",
      "\n",
      "Here's some code comparing eager, lazy, and GPU accelerated lazy computation.\n",
      "\n",
      "    \"\"\"Performing the same operations on the same data between three dataframes,\n",
      "    one with eager execution, one with lazy execution, and one with lazy execution\n",
      "    and GPU acceleration. Calculating the difference in execution speed between the\n",
      "    three.\n",
      "    From https://iaee.substack.com/p/gpu-accelerated-polars-intuitively\n",
      "    \"\"\"\n",
      "    \n",
      "    import polars as pl\n",
      "    import numpy as np\n",
      "    import time\n",
      "    \n",
      "    # Creating a large random DataFrame\n",
      "    num_rows = 20_000_000  # 20 million rows\n",
      "    num_cols = 10          # 10 columns\n",
      "    n = 10  # Number of times to repeat the test\n",
      "    \n",
      "    # Generate random data\n",
      "    np.random.seed(0)  # Set seed for reproducibility\n",
      "    data = {f\"col_{i}\": np.random.randn(num_rows) for i in range(num_cols)}\n",
      "    \n",
      "    # Defining a function that works for both lazy and eager DataFrames\n",
      "    def apply_transformations(df):\n",
      "        df = df.filter(pl.col(\"col_0\") > 0)  # Filter rows where col_0 is greater than 0\n",
      "        df = df.with_columns((pl.col(\"col_1\") * 2).alias(\"col_1_double\"))  # Double col_1\n",
      "        df = df.group_by(\"col_2\").agg(pl.sum(\"col_1_double\"))  # Group by col_2 and aggregate\n",
      "        return df\n",
      "    \n",
      "    # Variables to store total durations for eager and lazy execution\n",
      "    total_eager_duration = 0\n",
      "    total_lazy_duration = 0\n",
      "    total_lazy_GPU_duration = 0\n",
      "    \n",
      "    # Performing the test n times\n",
      "    for i in range(n):\n",
      "        print(f\"Run {i+1}/{n}\")\n",
      "    \n",
      "        # Create fresh DataFrames for each run (polars operations can be in-place, so ensure clean DF)\n",
      "        df1 = pl.DataFrame(data)\n",
      "        df2 = pl.DataFrame(data).lazy()\n",
      "        df3 = pl.DataFrame(data).lazy()\n",
      "    \n",
      "        # Measure eager execution time\n",
      "        start_time_eager = time.time()\n",
      "        eager_result = apply_transformations(df1)  # Eager execution\n",
      "        eager_duration = time.time() - start_time_eager\n",
      "        total_eager_duration += eager_duration\n",
      "        print(f\"Eager execution time: {eager_duration:.2f} seconds\")\n",
      "    \n",
      "        # Measure lazy execution time\n",
      "        start_time_lazy = time.time()\n",
      "        lazy_result = apply_transformations(df2).collect()  # Lazy execution\n",
      "        lazy_duration = time.time() - start_time_lazy\n",
      "        total_lazy_duration += lazy_duration\n",
      "        print(f\"Lazy execution time: {lazy_duration:.2f} seconds\")\n",
      "    \n",
      "        # Defining GPU Engine\n",
      "        gpu_engine = pl.GPUEngine(\n",
      "            device=0, # This is the default\n",
      "            raise_on_fail=True, # Fail loudly if we can't run on the GPU.\n",
      "        )\n",
      "    \n",
      "        # Measure lazy execution time\n",
      "        start_time_lazy_GPU = time.time()\n",
      "        lazy_result = apply_transformations(df3).collect(engine=gpu_engine)  # Lazy execution with GPU\n",
      "        lazy_GPU_duration = time.time() - start_time_lazy_GPU\n",
      "        total_lazy_GPU_duration += lazy_GPU_duration\n",
      "        print(f\"Lazy execution time: {lazy_GPU_duration:.2f} seconds\")\n",
      "    \n",
      "    # Calculating the average execution time\n",
      "    average_eager_duration = total_eager_duration / n\n",
      "    average_lazy_duration = total_lazy_duration / n\n",
      "    average_lazy_GPU_duration = total_lazy_GPU_duration / n\n",
      "    \n",
      "    #calculating how much faster lazy execution was\n",
      "    faster_1 = (average_eager_duration-average_lazy_duration)/average_eager_duration*100\n",
      "    faster_2 = (average_lazy_duration-average_lazy_GPU_duration)/average_lazy_duration*100\n",
      "    faster_3 = (average_eager_duration-average_lazy_GPU_duration)/average_eager_duration*100\n",
      "    \n",
      "    print(f\"\\nAverage eager execution time over {n} runs: {average_eager_duration:.2f} seconds\")\n",
      "    print(f\"Average lazy execution time over {n} runs: {average_lazy_duration:.2f} seconds\")\n",
      "    print(f\"Average lazy execution time over {n} runs: {average_lazy_GPU_duration:.2f} seconds\")\n",
      "    print(f\"Lazy was {faster_1:.2f}% faster than eager\")\n",
      "    print(f\"GPU was {faster_2:.2f}% faster than CPU Lazy and {faster_3:.2f}% faster than CPU eager\")\n",
      "\n",
      "And here's some of the results I saw\n",
      "\n",
      "    ...\n",
      "    Run 10/10\n",
      "    Eager execution time: 0.77 seconds\n",
      "    Lazy execution time: 0.70 seconds\n",
      "    Lazy execution time: 0.17 seconds\n",
      "    \n",
      "    Average eager execution time over 10 runs: 0.77 seconds\n",
      "    Average lazy execution time over 10 runs: 0.69 seconds\n",
      "    Average lazy execution time over 10 runs: 0.17 seconds\n",
      "    Lazy was 10.30% faster than eager\n",
      "    GPU was 74.78% faster than CPU Lazy and 77.38% faster than CPU eager\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.876\n",
      "Positive: 0.124\n",
      "Compound: 0.9793\n",
      "Body: Hey guys,\n",
      "\n",
      "I've been silent here the last month but many opportunities appeared!\n",
      "\n",
      "I runÂ [www.sportsjobs.online](http://www.sportsjobs.online/), a job board in that niche. I scan daily dozens of teams and companies.\n",
      "\n",
      "I'm constantly checking for jobs in the sports and gaming analytics industry. I'veÂ [posted recently in this community](https://www.reddit.com/r/datascience/comments/1evdmth/plenty_of_data_science_jobs_in_the_mls_nhl_nfl/)Â and had some good comments.\n",
      "\n",
      "Here is a summary with some hand picked ones!\n",
      "\n",
      "* [Lead Python Engineer (Sports Analytics) @ Singular](https://www.sportsjobs.online/jobs/recxR4T2FS314wmBC)\n",
      "* [Data Scientist & Analyst - Formula1 @ Formula 1](https://www.sportsjobs.online/jobs/receV2F7rzS5yHy9J)\n",
      "* [Senior Data Science Manager @ Houston Texans](https://www.sportsjobs.online/jobs/recVcMbnr5ITG2Gul)\n",
      "* [Senior Software Engineer - Formula1 @ McLaren Racing](https://www.sportsjobs.online/jobs/rec0NJNFTlE0DDyyo)\n",
      "* [Data Engineer, Hockey Analytics - NHL @ New Jersey Devils](https://www.sportsjobs.online/jobs/recmIjwV6Oq6LgxqK)\n",
      "* [Business Intelligence Intern @ Sacramento Kings](https://www.sportsjobs.online/jobs/recWS4dOp38tmrsgp)\n",
      "* [Senior Data Scientist @ Epic Games Store](https://www.sportsjobs.online/jobs/reczosFzj58DjB0UB)\n",
      "* [Soccer Data Scientist @ Swish Analytics](https://www.sportsjobs.online/jobs/recrryn1mupsnKJDc)\n",
      "* [Data Scientist @ Fliff](https://www.sportsjobs.online/jobs/recFYXjLSwMGpsqFN)\n",
      "\n",
      "  \n",
      "Actually these are from the last 10 days only! But in the last month we loaded around 200 jobs  including many in the MLS, some more in NFL and Formula 1 and others more in the software and data engineering side.\n",
      "\n",
      "I hope this helps someone!\n",
      "\n",
      "Negative: 0.075\n",
      "Neutral: 0.79\n",
      "Positive: 0.135\n",
      "Compound: 0.8769\n",
      "Body: I have month end data for about 75 variables (numeric and category factor, but mostly numeric) for the last 5 years.  I have a dependent variable that I'd like to understand the key drivers for, and be able to predict the probability of with new data.  Typically I would use a random forest or LASSO regression, and I'm struggling given the data's time series nature.  I understand random forest, and most normal regression models assume independent observations, but I have month end sequential data points.  \n",
      "\n",
      "So what should I do?  Should I just ignore the time series nature and run the models as-is?  I know there's models for everything, but I'm not familiar with another strong option to tackle this problem.\n",
      "\n",
      "Any help is appreciated, thanks!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.946\n",
      "Positive: 0.054\n",
      "Compound: 0.5845\n",
      "Body: I am working on a project that generates a cost estimation report. The report can be generated using LLM, but if we directly give the user query without some knowledge base, the LLM will hallucinates. \n",
      "For generating accurate results we need real world data. \n",
      "Where we can get this kind of data?\n",
      "Is common crawl an option? \n",
      "Does paid platforms like Apollo or any other provides such data? \n",
      "\n",
      "Negative: 0.081\n",
      "Neutral: 0.805\n",
      "Positive: 0.114\n",
      "Compound: 0.3939\n",
      "Body: Inspired by [this](https://www.reddit.com/r/ExperiencedDevs/s/5Yl0swfZaU) post in a separate community. \n",
      "\n",
      "What do you consider to be complex in your role? And what does complex mean when asked by interviewers on \"the toughest problem you solved\"? Or similarly when Elon asks about examples of exceptional work done.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.035\n",
      "Neutral: 0.9\n",
      "Positive: 0.064\n",
      "Compound: 0.9879\n",
      "Body: Hey folks,\n",
      "\n",
      "I'm a Sr. Analytics Data Scientist at a large tech firm (not FAANG) and I conduct about \\~3 interviews per week. I wanted to share myÂ transition to data science in case it helps other folks, as well as share my advice for how to nail the product analytics interviews. I also want to raise awareness that Product Analytics is a very viable and lucrative data science path. I'm not going to get into the distinction between analytics and data science/machine learning here. Just know that I don't do any predictive modeling, and instead do primarily AB testing, causal inference, and dashboarding/reporting. I do want to make one thing clear: This advice is primarily applicable to analytics roles in tech. It is probably not applicable for ML or Applied Scientist roles, or for fields other than tech. Analytics roles can be very lucrative, and the barrier to entry is lower than that for Machine Learning roles. The bar for coding and math is relatively low (you basically only need to know SQL, undergraduate statistics, and maybe beginner/intermediate Python). For ML and Applied Scientist roles, the bar for coding and math is much higher.Â \n",
      "\n",
      "Here is my path into analytics. Just FYI, I live in a HCOL city in the US.\n",
      "\n",
      "**Path to Data/Product Analytics**\n",
      "\n",
      "* 2014-2017 - Deloitte Consulting\n",
      "   * Role: Business Analyst, promoted to Consultant after 2 years\n",
      "   * Pay: Started at a base salary of $73k no bonus, ended at $89k no bonus.\n",
      "* 2017-2018: Non-FAANG tech company\n",
      "   * Role: Strategy Manager\n",
      "   * Pay: Base salary of $105k, 10% annual bonus. No equity\n",
      "* 2018-2020: Small start-up (\\~300 people)\n",
      "   * Role: Data Analyst. At the previous non-FAANG tech company, I worked a lot with the data analytics team. I realized that I couldn't do my job as a \"Strategy Manager\" without the data team because without them, I couldn't get any data. At this point, I realized that I wanted to move into a data role.\n",
      "   * Pay: Base salary of $100k. No bonus, paper money equity. Ended at $115k.\n",
      "   * Other: To get this role, I studied SQL on the side.\n",
      "* 2020-2022: Mid-sized start-up in the logistics space (\\~1000 people).\n",
      "   * Role: Business Intelligence Analyst II. Work was done using mainly SQL and Tableau\n",
      "   * Pay: Started at $100k base salary, ended at $150k through a series of one promotion to Data Scientist, Analytics and two \"market rate adjustments\". No bonus, paper equity.\n",
      "   * Also during this time, I completed a part time masters degree in Data Science. However, for \"analytics data science\" roles, in hindsight, the masters was unnecessary. The masters degree focused heavily on machine learning, but analytics roles in tech do very little ML.\n",
      "* 2022-current: Large tech company, not FAANG\n",
      "   * Role: Sr. Analytics Data Scientist\n",
      "   * Pay (RSUs numbers are based on the time I was given the RSUs): Started at $210k base salary with annual RSUs worth $110k. Total comp of $320k. Currently at $240k base salary, plus additional RSUs totaling to $270k per year. Total comp of $510k.\n",
      "   * I will mention that this comp is on the high end. I interviewed a bunch in 2022 and received 6 full-time offers for Sr. analytics roles and this was the second highest offer. The lowest was $185k base salary at a startup with paper equity.\n",
      "\n",
      "**How to pass tech analytics interviews**\n",
      "\n",
      "Unfortunately, I donâ€™t have much advice on how to get an interview. What Iâ€™ll say is to emphasize the following skills on your resume:\n",
      "\n",
      "* SQL\n",
      "* AB testing\n",
      "* Using data to influence decisions\n",
      "* Building dashboards/reports\n",
      "\n",
      "And de-emphasize model building. I have worked with Sr. Analytics folks in big tech that don't even know what a model is. The only models I build are the occasional linear regression for inference purposes.\n",
      "\n",
      "Assuming you get the interview, here is my advice on how to pass an analytics interview in tech.\n",
      "\n",
      "* **You have to be able to pass the SQL screen**. My current company, as well as other large companies such as Meta and Amazon, literally only test SQL as for as technical coding goes. This is pass/fail. You have to pass this. We get so many candidates that look great on paper and all say they are expert in SQL, but can't pass the SQL screen. Grind SQL interview questions until you can answer easy questions in <4 minutes, medium questions in <5 minutes, and hard questions in <7 minutes. This should let you pass 95% of SQL interviews for tech analytics roles.\n",
      "* You will likely be asked some case study type questions. To pass this, youâ€™ll likely need to know AB testing and have strong product sense, and maybe causal inference for senior/principal level roles.Â [This article by Interviewquery](https://www.interviewquery.com/p/data-science-case-study-interview-questions#data-analytics-case-study-questions)Â provides a lot of case question examples, although it doesnâ€™t provide sample answers (I have no affiliation with Interviewquery). All of them are relevant for tech analytics role case interviews except the Modeling and Machine Learning section.\n",
      "\n",
      "**Final notes**  \n",
      "It's really that simple (although not easy). In the past 2.5 years, I passed 11 out of 12 SQL screens by grinding 10-20 SQL questions per day for 2 weeks. I also practiced a bunch of product sense case questions, brushed up on my AB testing, and learned common causal inference techniques. As a result, I landed 6 offers out of 8 final round interviews. Please note that my above advice is not necessarily what is needed to be successful in tech analytics. It is advice for how to pass the tech analytics interviews.\n",
      "\n",
      "If anybody is interested in learning more about tech product analytics, or wants help on passing the tech analytics interview, just DM me. I wrote up a guide on how to pass analytics interviews because a lot of my classmates had asked me for advice. I don't think the sub-rules allow me to link it though, so DM me and I'll send it to you. I also have a Youtube channel where I solve mock SQL interview questions live. Thanks, I hope this is helpful.\n",
      "\n",
      "  \n",
      "Edit: Too many DMs. If I didn't respond, the guide and Youtube channel are in my reddit profile. I do try and respond to everybody, sorry if I didn't respond.\n",
      "\n",
      "Negative: 0.05\n",
      "Neutral: 0.883\n",
      "Positive: 0.067\n",
      "Compound: 0.3352\n",
      "Body: Weâ€™ve been tasked with creating a â€œsafe rangeâ€ for several of our KPIs that will make it easier for leadership to understand when metrics are performing as relatively expected. These KPIs are usually tracked weekly and monthly.\n",
      "\n",
      "We have carte blanche in terms of methodology. Not sure where to start. How would you approach this initially? Some sort of moving average and the upper and lower bounds being a standard dev from that average? Something more complex? \n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.693\n",
      "Positive: 0.307\n",
      "Compound: 0.7351\n",
      "Body: I've never understood why everything has to be capitalized. Just curious lmao\n",
      "\n",
      "SELECT \\*\n",
      "\n",
      "FROM\n",
      "\n",
      "WHERE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.918\n",
      "Positive: 0.082\n",
      "Compound: 0.5106\n",
      "Body: I am new to time-series analysis and learning it through the \"Forecasting: Principles and Practice (2nd ed)\" book. I can see several GitHub repos with the solutions of the book but quite outdated. I was wondering if the community has any recommendations for such solution repos using R and Python.\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.944\n",
      "Positive: 0.056\n",
      "Compound: 0.6696\n",
      "Body:  \n",
      "\n",
      "Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n",
      "\n",
      "* Learning resources (e.g. books, tutorials, videos)\n",
      "* Traditional education (e.g. schools, degrees, electives)\n",
      "* Alternative education (e.g. online courses, bootcamps)\n",
      "* Job search questions (e.g. resumes, applying, career prospects)\n",
      "* Elementary questions (e.g. where to start, what next)\n",
      "\n",
      "While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.694\n",
      "Positive: 0.306\n",
      "Compound: 0.296\n",
      "Body: Could you share your reasons why?\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.787\n",
      "Positive: 0.213\n",
      "Compound: 0.9872\n",
      "Body: NVIDIA is offering many free courses at its [Deep Learning Institute](https://nvda.ws/3X9i910). Some of my favourites\n",
      "\n",
      "1. [**Building RAG Agents with LLMs**](https://nvda.ws/3XpYrzo): This course will guide you through the practical deployment of an RAG agent system (how to connect external files like PDF to LLM).\n",
      "2. [**Generative AI Explained**](https://nvda.ws/3XpYrPU): In this no-code course, explore the concepts and applications of Generative AI and the challenges and opportunities present. *Great for GenAI beginners!*\n",
      "3. [**An Even Easier Introduction to CUDA**](https://nvda.ws/4dPktkQ)**:** The course focuses on utilizing NVIDIA GPUs to launch massively parallel CUDA kernels, enabling efficient processing of large datasets.\n",
      "4. [**Building A Brain in 10 Minutes:**](https://nvda.ws/3XtDhQQ) Explains the explores the biological inspiration for early neural networks. Good for Deep Learning beginners.\n",
      "\n",
      "I tried a couple of them and they are pretty good, especially the coding exercises for the **RAG framework** (how to connect external files to an LLM). Worth giving a try !!\n",
      "\n",
      "Negative: 0.0\n",
      "Neutral: 0.885\n",
      "Positive: 0.115\n",
      "Compound: 0.8402\n",
      "Body: I'm starting my 3rd year studying for a 4 year integrated MSci in Economics in the UK.   \n",
      "I've been choosing modules/courses that lean towards econometrics and data science, like Time Series, Web Scraping and Machine Learning.  \n",
      "I've already done some statistics and econometrics in my previous years as well as coding in Jupyter Notebooks and R, and I'll be starting SQL this year. Is this a good foundation for going for data science, or would you recommend a different career path?\n",
      "\n",
      "Negative: 0.073\n",
      "Neutral: 0.744\n",
      "Positive: 0.183\n",
      "Compound: 0.9535\n",
      "Body: Good Morning/Afternoon Everyone,\n",
      "\n",
      "I have been trying to get a job almost an year in the UK. My resume is shown here and i agree that this was not the first resume of mine, this one is the one i made 2 weeks ago. But i have been struggling to get interviews. I have gotten like 3 interviews in the entire 10 months of applying. Truly now i am starting to question that am i truly doing something wrong ?? I have tried to quantify as much as i can. Trying to show business impact and how profitable they can be. Trying to create relevant projects and even deploying them on cloud. Any sort of responses or tips would be highly appreciated.\n",
      "\n",
      "Thank you so much for reading this.\n",
      "\n",
      "[Apologies for the terrible screenshot quality.](https://preview.redd.it/lrhvvxorh4pd1.png?width=585&format=png&auto=webp&s=7c28a8a9df26be1e977ca12e5f50048018cb67db)\n",
      "\n",
      "\n",
      "\n",
      "Negative: 0.034\n",
      "Neutral: 0.928\n",
      "Positive: 0.037\n",
      "Compound: 0.0772\n",
      "Body: Hello everyone. I need to get a project done within the next week. Specifically I need to do a small project regarding anything about finance with Panel Data. I was thinking something about the rating of companies based on their performance but I donâ€™t know where I can find the data.\n",
      "\n",
      "Another problem is: I know nothing about Panel data. I already tried to read Econometric analysis of Panel Data by Baltagi but itâ€™s just too much math for me. Do you have any suggestion? If you have somthing with application in Python it would be even better \n",
      "\n",
      "Negative: 0.019\n",
      "Neutral: 0.888\n",
      "Positive: 0.093\n",
      "Compound: 0.9742\n",
      "Body: I may find myself in the position of moving from the USA to the UK in less than a year's time as my spouse is an academic who's going on the European (mostly UK) job market for academia.\n",
      "\n",
      "I effectively have the equivalent of a 1st in both my undergrad (including a STEM major) and my MS (data science), as well as 2 years of non-DS experience and 1 year of DS experience. I'm not sure about the visa situationâ€”either HPI or some sort of arrangement as my partner's spouseâ€”but assuming I can secure some kind of working visa, I've no clue about the UK job market. \n",
      "\n",
      "I've searched this sub but there aren't many results. I've had a few random conversations here and there with UK pals and other people who say the market is overall better in the UK than in the US. Obviously that comes with a variety of caveats regarding quality of life, salary, etc., which I'm aware of so not worried about that. I've taken a peek at Linkedin UK and most jobs are naturally centred around London with a variety of remote/hybrid/on-site. Unless my partner somehow manages a good post in London, though, I expect we'll be living in the midlands or north to get away from the London cost of living...\n",
      "\n",
      "Is the UK job market \"better\" than the USA in terms of time from first application to offer? I imagine part of the paradigm is that there are less candidates in competition as many are drawn to the USA's relatively fat checks. I'm just trying to get a feel for what things are like right now in the UK since I otherwise have no context about jobs.\n",
      "\n",
      "TIA!\n",
      "\n",
      "Negative: 0.009\n",
      "Neutral: 0.88\n",
      "Positive: 0.112\n",
      "Compound: 0.9878\n",
      "Body: Note - I live in Canada, we just donâ€™t have a flair for that. \n",
      "\n",
      "Hello all,\n",
      "\n",
      "I have an annual review in a little over a week and I'm feeling like my career path lacks direction. \n",
      "\n",
      "I've worked at my company for 3.5 years as a Data Migration Analyst, and was promoted to a Senior Data Migration Analyst about 8 months ago. My day-to-day generally involves:\n",
      "\n",
      "* Migrating customer data to our software (working with SQL and JSON files)\n",
      "* Attending daily Dev-Ops meetings and doing tasks in that area (ie. shell scripting, database management) on both AWS and Azure, although we are moving exclusively to AWS shortly \n",
      "* Lead a team of 3 other Data Migration Analysts\n",
      "* Doing custom requests on customer DB's (SQL scripting for their large updates)\n",
      "* Handle miscellaneous requests for other departments\n",
      "\n",
      "I did my undergraduate degree in Data Analytics & Finance, with minors in CS and IT. I also have a Masters in Data Science. \n",
      "\n",
      "My dilemma is that I feel that I am a master of none. I have a lot of general skills, such as SQL, Cloud Technologies and Database Management, but I'm not an expert. I also have a strong background in stats, ML and python/r programming from my undergrad/graduate degrees - all of which are not being used. \n",
      "\n",
      "I enjoy what I do, but I want to follow a path where I'll make more money and have hard skills that contribute to a strong resume. More importantly, I want a job that has strong prospects in the future as well. \n",
      "\n",
      "I'm currently trying to weigh my options:\n",
      "\n",
      "1. Deep dive into cloud technologies and become an expert in cloud engineering or something along those lines\n",
      "2. Improve my python programming skills and focus in data engineering\n",
      "3. Try to get back to my roots and find work in DA/DS/BI since it's the bulk of what I studied\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for post in posts:\n",
    "    sentiment_scores = analyzer.polarity_scores(post['processed_body'])\n",
    "    print(f\"Negative: {sentiment_scores['neg']}\")\n",
    "    print(f\"Neutral: {sentiment_scores['neu']}\")\n",
    "    print(f\"Positive: {sentiment_scores['pos']}\")\n",
    "    print(f\"Compound: {sentiment_scores['compound']}\")\n",
    "    print(f\"Body: {post['processed_body']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "insert_query = f\"\"\"\n",
    "INSERT INTO {schema}.data_science_posts (post_id, title, author, subreddit, tags, title_link, timestamp, processed_body, sentiment)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "ON CONFLICT (post_id) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "for post in posts:\n",
    "    cur.execute(insert_query, (\n",
    "        post['post_id'], post['title'], post['author'], post['subreddit'], \n",
    "        [post['tags']] if post['tags'] else None, post['url'], post['created'], \n",
    "        post['processed_body'], post['sentiment']\n",
    "    ))\n",
    "\n",
    "print(\"Data inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_reviews(df, sentiment_label):\n",
    "    if sentiment_label == 'positive':\n",
    "        top_reviews = df[df['sentiment'] == 'positive'].sort_values(by='compound', ascending=False).head(3)\n",
    "    elif sentiment_label == 'negative':\n",
    "        top_reviews = df[df['sentiment'] == 'negative'].sort_values(by='compound').head(3)\n",
    "    else:\n",
    "        neutral_df = df[df['sentiment'] == 'neutral']\n",
    "        top_reviews = neutral_df.iloc[(neutral_df['compound'] - 0).abs().argsort()[:3]]\n",
    "    \n",
    "    print(f\"\\nTop 3 {sentiment_label.capitalize()} Reviews:\")\n",
    "    for idx, row in top_reviews.iterrows():\n",
    "        print(f\"\\nReview {idx+1}:\")\n",
    "        print(f\"Compound Score: {row['compound']}\")\n",
    "        print(row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All posts inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "insert_query = \"\"\"\n",
    "INSERT INTO data_science_posts (post_id, title, author, subreddit, title_link, timestamp, processed_body, sentiment)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "ON CONFLICT (post_id) DO NOTHING;\n",
    "\"\"\"\n",
    "with connection, connection.cursor() as cursor:\n",
    "    for post in posts:\n",
    "        cursor.execute(insert_query, (\n",
    "            post['post_id'],\n",
    "            post['title'],\n",
    "            post['author'],\n",
    "            post['subreddit'],\n",
    "            post['url'],\n",
    "            post['created'],\n",
    "            post['processed_body'],\n",
    "            post['sentiment']\n",
    "        ))\n",
    "\n",
    "print(\"All posts inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post ID: 1fxz22n\n",
      "Title: Weekly Entering & Transitioning - Thread 07 Oct, 2024 - 14 Oct, 2024\n",
      "Author: AutoModerator\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-06 23:01:31\n",
      "body:  \n",
      "\n",
      "Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n",
      "\n",
      "* Learning resources (e.g. books, tutorials, videos)\n",
      "* Traditional education (e.g. schools, degrees, electives)\n",
      "* Alternative education (e.g. online courses, bootcamps)\n",
      "* Job search questions (e.g. resumes, applying, career prospects)\n",
      "* Elementary questions (e.g. where to start, what next)\n",
      "\n",
      "While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Post ID: 1fzswk1\n",
      "Title: I created a 6-week SQL for data science roadmap as a public Github repo\n",
      "Author: avourakis\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-09 09:36:10\n",
      "body: I created this roadmap to guide you through mastering SQL in aboutÂ 6 weeksÂ (or sooner if you have the time and are motivated) for free, focusing specifically on skills essential for aspiringÂ Data ScientistsÂ (orÂ Data Analysts)\n",
      "\n",
      "Each section points you to specific resources, mostly YouTube videos and articles, to help you learn each concept.\n",
      "\n",
      "[https://github.com/andresvourakis/free-6-week-sql-roadmap-data-science](https://github.com/andresvourakis/free-6-week-sql-roadmap-data-science)\n",
      "\n",
      "Btw, Iâ€™m a data scientist with 7 years of experience in tech. Iâ€™ve been working with SQL ever since I started my career.\n",
      "\n",
      "I hope this helps those of you just getting started or in need of refresher ðŸ™\n",
      "\n",
      "P.S. Iâ€™m creating a similar roadmap for Python, which hopefully will be ready in a couple of days\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Post ID: 1g01l33\n",
      "Title: SQL queries that group by number\n",
      "Author: coronnial\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-09 15:41:19\n",
      "body: I wanted to know if generally people use group by with the numbers instead of the column names. Is this something old school or just bad practice? It makes it so much harder to read.\n",
      "Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Post ID: 1fzw393\n",
      "Title: Need help on analysis of AI performance, compute and time.\n",
      "Author: PianistWinter8293\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-09 11:50:22\n",
      "body: \n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Post ID: 1g07nvw\n",
      "Title: I linked AI Performance Data with Compute Size Data and analyzed over Time\n",
      "Author: PianistWinter8293\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-09 20:27:28\n",
      "body: \n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Post ID: 1fzh90w\n",
      "Title: Which position should I join? (Palantir Developer vs BI Analyst)\n",
      "Author: naive_byes\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-08 21:26:32\n",
      "body: I have recently received two offers from two different companies. Same pay and remote.  \n",
      "  \n",
      "Company A (Fortune 500)  \n",
      "Role - **Palantir Application Developer**  \n",
      "In this role, I have to collaborate with senior leaders of the company and develop **Palantir applications** to solve their problems ...and it will be more of a Data Engineer sort of work. However, I am scared as there are not enough palantir-related jobs in the market. The software is costly and is thus not adopted by a lot of organizations. However, the manager is saying that I will get huge exposure to the business as I will be interacting with the senior leadership to understand the business problems.\n",
      "\n",
      "Company B (A health system)  \n",
      "Role - **BI Analyst**  \n",
      "In this role, I will lead the data science collaboration of the health system and there are opportunities to grow into the data science team as well. The company doesn't have a proper data science team thus there is a lot of room I suppose. They use **Dataiku** platform to apply machine learning.\n",
      "\n",
      "Which role should I choose?\n",
      "Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Post ID: 1fznew0\n",
      "Title: Does business dictate what models or methodology to use?\n",
      "Author: jaegarbong\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-09 04:30:56\n",
      "body: Hey guys,\n",
      "\n",
      "I am working on a forecasting project and after two restarts , I am getting some weird vibes from my business SPOC.\n",
      "\n",
      "Not only he is not giving me enough business side details to expand on my features, he is dictating what models to use. For .e.g. I got an email from him saying to use MLR, DT, RF, XGB, LGBM, CatBoost for forecasting using ML. Also, he wants me to use ARIMA/SARIMAX for certain classes of SKUs.\n",
      "\n",
      "The problem seems to be that there is no quantitative KPI for stopping the experimentation. Just the visual analysis of results.\n",
      "\n",
      "For e.g my last experiment got rejected because 3 rows of forecasts were off the mark (by hundreds) out of 10K rows generated in the forecast table. Since the forecast was for highly irregular and volatile SKUs, my model was forecasting within what seemed to be an acceptable error range. If actual sales were 100, my model was showing 92 or 112 etc.\n",
      "\n",
      "Since this is my first major model building on a massive scale, I was wondering if things are like this.\n",
      "Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Post ID: 1fzi4an\n",
      "Title: does anyone use Posit Connect?\n",
      "Author: bee_advised\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-08 22:13:56\n",
      "body: I'm curious what companies out there are using Posit's cloud tools like Workbench, Connect and Posit Package Manager and if anyone has used them.\n",
      "\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Post ID: 1fyrawz\n",
      "Title: A guide to passing the A/B test interview question in tech companies\n",
      "Author: productanalyst9\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-07 23:08:05\n",
      "body: Hey all,\n",
      "\n",
      "I'm a Sr. Analytics Data Scientist at a large tech firm (not FAANG) and I conduct about \\~3 interviews per week. I wanted to share my advice on how to pass A/B test interview questions as this is an area I commonly see candidates get dinged. Hope it helps.\n",
      "\n",
      "Product analytics and data scientist interviews at tech companies often include an A/B testing component. Here is my framework on how to answer A/B testing interview questions. Please note that this is not necessarily a guide to design a good A/B test. Rather, it is a guide to help you convince an interviewer that you know how to design A/B tests.\n",
      "\n",
      "**A/B Test Interview Framework**\n",
      "\n",
      "Imagine during the interview that you get asked â€œWalk me through how you would A/B test this new feature?â€. This framework will help you pass these types of questions.\n",
      "\n",
      "**Phase 1: Set the context for the experiment. Why do we want to AB test, what is our goal, what do we want to measure?**\n",
      "\n",
      "1. The first step is to clarify the purpose and value of the experiment with the interviewer. Is it even worth running an A/B test? Interviewers want to know that the candidate can tie experiments to business goals.\n",
      "2. Specify what exactly is the treatment, and what hypothesis are we testing? Too often I see candidates fail to specify what the treatment is, and what is the hypothesis that they want to test. Itâ€™s important to spell this out for your interviewer.Â \n",
      "3. After specifying the treatment and the hypothesis, you need to define the metrics that you will track and measure.\n",
      "   * Success metrics: Identify at least 2-3 candidate success metrics. Then narrow it down to one and propose it to the interviewer to get their thoughts.\n",
      "   * Guardrail metrics: Guardrail metrics are metrics that you do not want to harm. You donâ€™t necessarily want to improve them, but you definitely donâ€™t want to harm them. Come up with 2-4 of these.\n",
      "   * Tracking metrics: Tracking metrics help explain the movement in the success metrics. Come up with 1-4 of these.\n",
      "\n",
      "**Phase 2: How do we design the experiment to measure what we want to measure?**\n",
      "\n",
      "1. Now that you have your treatment, hypothesis, and metrics, the next step is to determine the unit of randomization for the experiment, and when each unit will enter the experiment. You should pick a unit of randomization such that you can measure success your metrics, avoid interference and network effects, and consider user experience.\n",
      "   * As a simple example, letâ€™s say you want to test a treatment that changes the color of the checkout button on an ecommerce website from blue to green. How would you randomize this? You could randomize at the user level and say that every person that visits your website will be randomized into the treatment or control group. Another way would be to randomize at the session level, or even at the checkout page level.Â \n",
      "   * When each unit will enter the experiment is also important. Using the example above, you could have a person enter the experiment as soon as they visit the website. However, many users will not get all the way to the checkout page so you will end up with a lot of users who never even got a chance to see your treatment, which will dilute your experiment. In this case, it might make sense to have a person enter the experiment once they reach the checkout page. You want to choose your unit of randomization and when they will enter the experiment such that you have minimal dilution. In a perfect world, every unit would have the chance to be exposed to your treatment.\n",
      "2. The next step is to conduct a power analysis to determine the number of observations required and how long to run the experiment. You can either state that you would conduct a power analysis using an alpha of 0.05 and power of 80%, or ask the interviewer of the company has standards you should use.\n",
      "   * Iâ€™m not going to go into how to calculate power here, but know that in any ABÂ  test interview question, you will have to mention power. For some companies, and in junior roles, just mentioning this will be good enough. Other companies, especially for more senior roles, might ask you more specifics about how to calculate power.Â \n",
      "3. Next, you need to determine which statistical test(s) you will use to analyze the results. Is a simple t-test sufficient, or do you need quasi-experimental techniques like difference in differences? Do you require heteroskedastic robust standard errors or clustered standard errors?\n",
      "   * The t-test and z-test of proportions are two of the most common tests.\n",
      "   * If your unit of randomization is larger than your analysis unit, you may need to adjust how you calculate your standard errors.\n",
      "   * You might be thinking â€œwhy would I need to use difference-in-difference in an AB testâ€? In my experience, this is common when doing a geography based randomization on a relatively small sample size. Letâ€™s say that you want to randomize by city in the state of California. Itâ€™s likely that even though you are randomizing which cities are in the treatment and control groups, that your two groups will have pre-existing biases. A common solution is to use difference-in-difference. Iâ€™m not saying this is right or wrong, but itâ€™s a common solution that I have seen in tech companies.\n",
      "4. Final considerations for the experiment design:Â \n",
      "   * Are you testing multiple metrics? If so, account for that in your analysis. A really common academic answer is the Bonferonni correction. I've never seen anyone use it in real life though, because it is too conservative. A more common way is to control the False Discovery Rate. You can google this. Alternatively, the book [Trustworthy Online Controlled Experiments](https://amzn.to/4dzXyZP) by Ron Kohavi discusses how to do this (note: this is an affiliate link).Â \n",
      "   * Do any stakeholders need to be informed about the experiment?Â \n",
      "   * Are there any novelty effects or change aversion that could impact interpretation?\n",
      "\n",
      "**Phase 3:** **The experiment is over. Now what?**\n",
      "\n",
      "1. After you â€œrunâ€ the A/B test, you now have some data. Consider what recommendations you can make from them. What insights can you derive to take actionable steps for the business? Speaking to this will earn you brownie points with the interviewer.\n",
      "   * For example, can you think of some useful ways to segment your experiment data to determine whether there were heterogeneous treatment effects?\n",
      "\n",
      "**Common follow-up questions, or â€œgotchasâ€**\n",
      "\n",
      "These are common questions that interviewers will ask to see if you really understand A/B testing.\n",
      "\n",
      "* Letâ€™s say that you are mid-way through running your A/B test and the performance starts to get worse. It had a strong start but now your success metric is degrading. Why do you think this could be?\n",
      "   * A common answer is novelty effect\n",
      "* Letâ€™s say that your AB test is concluded and your chosen p-value cutoff is 0.05. However, your success metric has a p-value of 0.06. What do you do?\n",
      "   * Some options are: Extend the experiment. Run the experiment again.\n",
      "   * You can also say that you would discuss the risk of a false positive with your business stakeholders. It may be that the treatment doesnâ€™t have much downside, so the company is OK with rolling out the feature, even if there is no true improvement. However, this is a discussion that needs to be had with all relevant stakeholders and as a data scientist or product analyst, you need to help quantify the risk of rolling out a false positive treatment.\n",
      "* Your success metric was stat sig positive, but one of your guardrail metrics was harmed. What do you do?\n",
      "   * Investigate the cause of the guardrail metric dropping. Once the cause is identified, work with the product manager or business stakeholders to update the treatment such that hopefully the guardrail will not be harmed, and run the experiment again.\n",
      "   * Alternatively, see if there is a segment of the population where the guardrail metric was not harmed. Release the treatment to only this population segment.\n",
      "* Your success metric ended up being stat sig negative. How would you diagnose this?Â \n",
      "\n",
      "I know this is really long but honestly, most of the steps I listed could be an entire blog post by itself. If you don't understand anything, I encourage you to do some more research about it, or get the book that I linked above (I've read it 3 times through myself). Lastly, don't feel like you need to be an A/B test expert to pass the interview. We hire folks who have no A/B testing experience but can demonstrate framework of designing AB tests such as the one I have just laid out. Good luck!\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Post ID: 1fz6o4i\n",
      "Title: The Nobel Prize in Physics 2024 was awarded to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"\n",
      "Author: Cheap_Scientist6984\n",
      "Subreddit: datascience\n",
      "Timestamp: 2024-10-08 13:19:00\n",
      "body: \n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "select_query = \"\"\"\n",
    "SELECT post_id, title, author, subreddit, timestamp, processed_body, sentiment\n",
    "FROM data_science_posts\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(select_query)\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(f\"Post ID: {row[0]}\")\n",
    "    print(f\"Title: {row[1]}\")\n",
    "    print(f\"Author: {row[2]}\")\n",
    "    print(f\"Subreddit: {row[3]}\")\n",
    "    print(f\"Timestamp: {row[4]}\")\n",
    "    print(f\"body: {row[5]}\")\n",
    "    print(f\"Sentiment: {row[6]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'processed_body_tsv_gin' and 'processed_body_tsv_gist' columns to the data_science_posts table.\n"
     ]
    }
   ],
   "source": [
    "add_tsvector_columns_query = f\"\"\"\n",
    "ALTER TABLE {schema}.data_science_posts\n",
    "ADD COLUMN processed_body_tsv_gin TSVECTOR,\n",
    "ADD COLUMN processed_body_tsv_gist TSVECTOR;\n",
    "\"\"\"\n",
    "\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(add_tsvector_columns_query)\n",
    "\n",
    "print(\"Added 'processed_body_tsv_gin' and 'processed_body_tsv_gist' columns to the data_science_posts table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_TRIGGER=f\"\"\"\n",
    "--TRIGGER\n",
    "CREATE TRIGGER tsv_gin_update \n",
    "    BEFORE INSERT OR UPDATE\n",
    "    ON {schema}.data_science_posts \n",
    "    FOR EACH ROW \n",
    "    EXECUTE PROCEDURE \n",
    "    tsvector_update_trigger(processed_body_tsv_gin, 'pg_catalog.english', processed_body);\n",
    "\n",
    "CREATE TRIGGER tsv_gist_update \n",
    "    BEFORE INSERT OR UPDATE\n",
    "    ON {schema}.data_science_posts \n",
    "    FOR EACH ROW \n",
    "    EXECUTE PROCEDURE \n",
    "    tsvector_update_trigger(processed_body_tsv_gist, 'pg_catalog.english', processed_body);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(CREATE_TRIGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_INDEX=f\"\"\"\n",
    "-------------------------\n",
    "-- Create Indexes\n",
    "-------------------------\n",
    "\n",
    "-- GIN Index on processed_body (for trigram search)\n",
    "-- CREATE EXTENSION pg_trgm;  -- Ensure this is done by DB Admin\n",
    "CREATE INDEX data_science_posts_processed_body\n",
    "ON {schema}.data_science_posts USING GIN(processed_body gin_trgm_ops);\n",
    "\n",
    "-- GIN Index on processed_body_tsv_gin for full-text search\n",
    "CREATE INDEX data_science_posts_processed_body_tsv_gin\n",
    "ON {schema}.data_science_posts USING GIN(processed_body_tsv_gin);\n",
    "\n",
    "-- GIST Index on processed_body_tsv_gist for full-text search\n",
    "CREATE INDEX data_science_posts_processed_body_tsv_gist\n",
    "ON {schema}.data_science_posts USING GIST(processed_body_tsv_gist);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(CREATE_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pg_trgm extension created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_extension_query = \"CREATE EXTENSION IF NOT EXISTS pg_trgm;\"\n",
    "\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(create_extension_query)\n",
    "\n",
    "print(\"pg_trgm extension created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1\n",
      "Title: Weekly Entering & Transitioning - Thread 07 Oct, 2024 - 14 Oct, 2024\n",
      "Author: AutoModerator\n",
      "Subreddit: datascience\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "ID: 2\n",
      "Title: I created a 6-week SQL for data science roadmap as a public Github repo\n",
      "Author: avourakis\n",
      "Subreddit: datascience\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "ID: 3\n",
      "Title: SQL queries that group by number\n",
      "Author: coronnial\n",
      "Subreddit: datascience\n",
      "Sentiment: negative\n",
      "--------------------------------------------------\n",
      "ID: 4\n",
      "Title: Need help on analysis of AI performance, compute and time.\n",
      "Author: PianistWinter8293\n",
      "Subreddit: datascience\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "ID: 5\n",
      "Title: I linked AI Performance Data with Compute Size Data and analyzed over Time\n",
      "Author: PianistWinter8293\n",
      "Subreddit: datascience\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "ID: 6\n",
      "Title: Which position should I join? (Palantir Developer vs BI Analyst)\n",
      "Author: naive_byes\n",
      "Subreddit: datascience\n",
      "Sentiment: negative\n",
      "--------------------------------------------------\n",
      "ID: 7\n",
      "Title: Does business dictate what models or methodology to use?\n",
      "Author: jaegarbong\n",
      "Subreddit: datascience\n",
      "Sentiment: negative\n",
      "--------------------------------------------------\n",
      "ID: 8\n",
      "Title: does anyone use Posit Connect?\n",
      "Author: bee_advised\n",
      "Subreddit: datascience\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "ID: 9\n",
      "Title: A guide to passing the A/B test interview question in tech companies\n",
      "Author: productanalyst9\n",
      "Subreddit: datascience\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "ID: 10\n",
      "Title: The Nobel Prize in Physics 2024 was awarded to John J. Hopfield and Geoffrey E. Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"\n",
      "Author: Cheap_Scientist6984\n",
      "Subreddit: datascience\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "select_query = f\"\"\"\n",
    "SELECT id, title, author, subreddit, sentiment\n",
    "FROM {schema}.data_science_posts\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(select_query)\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(f\"ID: {row[0]}\")\n",
    "    print(f\"Title: {row[1]}\")\n",
    "    print(f\"Author: {row[2]}\")\n",
    "    print(f\"Subreddit: {row[3]}\")\n",
    "    print(f\"Sentiment: {row[4]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: After you have loaded data from a subreddit, choose a few more subreddit and load those!\n",
    "\n",
    "Add cells if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code in this cell\n",
    "## ------------------------\n",
    "subreddits = ['NLP', 'datascienceproject', 'machinelearning', 'LanguageTechnology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_sentiment(text):\n",
    "    sentiment_dict = analyzer.polarity_scores(text)\n",
    "    return sentiment_dict['compound']\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit_name in subreddits:\n",
    "    posts = []\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    for post in subreddit.hot(limit=100):\n",
    "        posts.append({\n",
    "            'id': post.id,\n",
    "            'title': post.title,\n",
    "            'author': str(post.author),\n",
    "            'subreddit': post.subreddit.display_name,\n",
    "            'url': post.url,\n",
    "            'created': datetime.datetime.fromtimestamp(post.created),\n",
    "            'processed_body': preprocess_text(post.selftext),\n",
    "            'sentiment': get_vader_sentiment(post.selftext)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from subreddit 'LanguageTechnology' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "insert_query = f\"\"\"\n",
    "    INSERT INTO {schema}.data_science_posts (post_id, title, author, subreddit, title_link, timestamp, processed_body, sentiment)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (post_id) DO NOTHING;\n",
    "    \"\"\"\n",
    "    \n",
    "with connection, connection.cursor() as cursor:\n",
    "    for post in posts:\n",
    "        cursor.execute(insert_query, (\n",
    "            post['id'],\n",
    "            post['title'],\n",
    "            post['author'],\n",
    "            post['subreddit'],\n",
    "            post['url'],\n",
    "            post['created'],\n",
    "            post['processed_body'],\n",
    "            post['sentiment']\n",
    "        ))\n",
    "\n",
    "print(f\"Data from subreddit '{subreddit_name}' loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In part II, we will search your database as `dsa_ro_user user`. To prepare your DB to be read, you will need to grant the dsa_ro_user schema access and select privileges on your table.\n",
    "\n",
    "```SQL\n",
    "GRANT USAGE ON SCHEMA <your schema> TO dsa_ro_user;  -- NOTE: change to your schema\n",
    "GRANT SELECT ON <your table> TO dsa_ro_user;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRANT_ACCESS=f\"\"\"\n",
    "GRANT USAGE ON SCHEMA {schema} TO dsa_ro_user;\n",
    "GRANT SELECT ON {schema}.data_science_posts TO dsa_ro_user;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(GRANT_ACCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit and close the connection\n",
    "connection.commit()\n",
    "cur.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
